{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wandb\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.14.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/kuko/Developer/School/NS/assignment-1/wandb/run-20230320_003713-8nvz27y9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ns-super-team/assignment-1/runs/8nvz27y9' target=\"_blank\">pytorch</a></strong> to <a href='https://wandb.ai/ns-super-team/assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ns-super-team/assignment-1' target=\"_blank\">https://wandb.ai/ns-super-team/assignment-1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ns-super-team/assignment-1/runs/8nvz27y9' target=\"_blank\">https://wandb.ai/ns-super-team/assignment-1/runs/8nvz27y9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"assignment-1\",\n",
    "    name=\"pytorch\",\n",
    "    reinit=True,\n",
    "    \n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "        \"learning_rate\": 1e-3,\n",
    "        \"dataset\": \"Titanic\",\n",
    "        \"epochs\": 500,\n",
    "        \"loss\": \"binary cross-entropy\",\n",
    "        \"optimizer\": \"Adam\"\n",
    "    }\n",
    ")\n",
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "# device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "device = 'cpu'\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch implementation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TitanicDataset(Dataset):\n",
    "    def __init__(self, data_dir):\n",
    "        self.labels = pd.read_csv(os.path.join(data_dir, 'labels.csv')).to_numpy(dtype='float32')\n",
    "        self.data = pd.read_csv(os.path.join(data_dir, 'data.csv')).to_numpy(dtype='float32')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # print(self.data[idx])\n",
    "        data = torch.from_numpy(self.data[idx])\n",
    "        label = torch.from_numpy(self.labels[idx])\n",
    "        # data = self.data[idx]\n",
    "        # label = self.labels[idx]\n",
    "\n",
    "        return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TitanicDataset(data_dir='data/train/')\n",
    "val_data = TitanicDataset(data_dir='data/val/')\n",
    "test_data = TitanicDataset(data_dir='data/test/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  1.0000,  49.0000,   1.0000,   1.0000, 110.8833]) tensor([0.])\n"
     ]
    }
   ],
   "source": [
    "features, label = train_data.__getitem__(idx=0)\n",
    "print(features, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_data, batch_size=16, shuffle=True)\n",
    "val_dataloader = DataLoader(val_data, batch_size=16, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=16, shuffle=True)\n",
    "\n",
    "# train_x, train_y = next(iter(train_dataloader))\n",
    "# print(train_x.shape, train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([16, 5]) torch.Size([16, 1])\n"
     ]
    }
   ],
   "source": [
    "for batch, (x, y) in enumerate(train_dataloader):\n",
    "    print(batch, x.shape, y.shape)\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (linear): Sequential(\n",
      "    (0): Linear(in_features=5, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=32, out_features=1, bias=True)\n",
      "    (7): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # self.flatten = nn.Flatten()\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(5, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = self.flatten(x)\n",
    "        logits = self.linear(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "wandb.config['model'] = model.__dict__['_modules']['linear']\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.watch(model, loss_fn, log='all', log_freq=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred:  tensor([0.5573], grad_fn=<SigmoidBackward0>)\n",
      "loss:  tensor(0.8149, grad_fn=<BinaryCrossEntropyBackward0>)\n"
     ]
    }
   ],
   "source": [
    "features, label = train_data.__getitem__(idx=0)\n",
    "features = features.to(device)\n",
    "label = label.to(device)\n",
    "\n",
    "pred = model(features)\n",
    "print('pred: ', pred)\n",
    "print('loss: ', loss_fn(pred, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_acc(y_pred, y) -> float:\n",
    "    y_pred = y_pred>0.5\n",
    "\n",
    "    # print(((y == y_pred).sum().item()))\n",
    "    return ((y == y_pred).sum().item())/len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_graph = []\n",
    "acc_graph = []\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer, epoch):\n",
    "    curr = 0\n",
    "    avg_acc = 0\n",
    "    avg_loss = 0\n",
    "    model.train()\n",
    "\n",
    "    for x, y in dataloader:\n",
    "        curr += len(y)\n",
    "\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        # Get prediction and compute loss\n",
    "        y_pred = model(x)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        avg_loss += loss.item()\n",
    "\n",
    "        # Update parameters\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # correct = (pred == y).type(torch.float).sum().item()/len(y)\n",
    "        avg_acc += calculate_acc(y_pred, y)\n",
    "\n",
    "        # print(f'loss: {loss.item():>5f} acc: {100*correct:>5f} | {curr}/{len(dataloader.dataset)}')\n",
    "\n",
    "    avg_acc /= len(dataloader)\n",
    "    avg_loss /= len(dataloader)\n",
    "    loss_graph.append(avg_loss)\n",
    "    acc_graph.append(avg_acc)\n",
    "    \n",
    "    # if (epoch % 10):\n",
    "    print(f'avg loss: {avg_loss:>5f} avg acc: {avg_acc:>5f}')\n",
    "    \n",
    "    wandb.log({'loss': avg_loss})\n",
    "    wandb.log({'acc': avg_acc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss_graph = []\n",
    "val_acc_graph = []\n",
    "\n",
    "def val(dataloader, model, loss_fn, epoch):    \n",
    "    model.eval()\n",
    "    avg_loss = 0\n",
    "    avg_acc = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            y_pred = model(x)\n",
    "            avg_loss += loss_fn(y_pred, y).item()\n",
    "            avg_acc += calculate_acc(y_pred, y)\n",
    "    \n",
    "    avg_loss /= len(dataloader)\n",
    "    avg_acc /= len(dataloader)\n",
    "\n",
    "    val_loss_graph.append(avg_loss)\n",
    "    val_acc_graph.append(avg_acc)\n",
    "    \n",
    "    # if (epoch % 10):\n",
    "    print(f'avg val loss: {avg_loss:>5f} avg val acc: {avg_acc:>5f}')\n",
    "    \n",
    "    wandb.log({'val_loss': avg_loss})\n",
    "    wandb.log({'val_acc': avg_acc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "avg loss: 0.632256 avg acc: 0.666667\n",
      "avg val loss: 0.645051 avg val acc: 0.620833\n",
      "-------------------------------\n",
      "Epoch 2\n",
      "avg loss: 0.613280 avg acc: 0.681624\n",
      "avg val loss: 0.637819 avg val acc: 0.686111\n",
      "-------------------------------\n",
      "Epoch 3\n",
      "avg loss: 0.619711 avg acc: 0.698184\n",
      "avg val loss: 0.618763 avg val acc: 0.629167\n",
      "-------------------------------\n",
      "Epoch 4\n",
      "avg loss: 0.603616 avg acc: 0.705128\n",
      "avg val loss: 0.622684 avg val acc: 0.650000\n",
      "-------------------------------\n",
      "Epoch 5\n",
      "avg loss: 0.603434 avg acc: 0.694979\n",
      "avg val loss: 0.616946 avg val acc: 0.651389\n",
      "-------------------------------\n",
      "Epoch 6\n",
      "avg loss: 0.604171 avg acc: 0.697115\n",
      "avg val loss: 0.629434 avg val acc: 0.672222\n",
      "-------------------------------\n",
      "Epoch 7\n",
      "avg loss: 0.606384 avg acc: 0.714744\n",
      "avg val loss: 0.656078 avg val acc: 0.608333\n",
      "-------------------------------\n",
      "Epoch 8\n",
      "avg loss: 0.605234 avg acc: 0.676816\n",
      "avg val loss: 0.636416 avg val acc: 0.690278\n",
      "-------------------------------\n",
      "Epoch 9\n",
      "avg loss: 0.598727 avg acc: 0.725427\n",
      "avg val loss: 0.604539 avg val acc: 0.741667\n",
      "-------------------------------\n",
      "Epoch 10\n",
      "avg loss: 0.602843 avg acc: 0.711538\n",
      "avg val loss: 0.611615 avg val acc: 0.700000\n",
      "-------------------------------\n",
      "Epoch 11\n",
      "avg loss: 0.587557 avg acc: 0.708333\n",
      "avg val loss: 0.614443 avg val acc: 0.697222\n",
      "-------------------------------\n",
      "Epoch 12\n",
      "avg loss: 0.588761 avg acc: 0.723291\n",
      "avg val loss: 0.589071 avg val acc: 0.736111\n",
      "-------------------------------\n",
      "Epoch 13\n",
      "avg loss: 0.583854 avg acc: 0.724893\n",
      "avg val loss: 0.592412 avg val acc: 0.700000\n",
      "-------------------------------\n",
      "Epoch 14\n",
      "avg loss: 0.570896 avg acc: 0.719551\n",
      "avg val loss: 0.590800 avg val acc: 0.697222\n",
      "-------------------------------\n",
      "Epoch 15\n",
      "avg loss: 0.578433 avg acc: 0.738248\n",
      "avg val loss: 0.576573 avg val acc: 0.736111\n",
      "-------------------------------\n",
      "Epoch 16\n",
      "avg loss: 0.565323 avg acc: 0.726496\n",
      "avg val loss: 0.594266 avg val acc: 0.720833\n",
      "-------------------------------\n",
      "Epoch 17\n",
      "avg loss: 0.577017 avg acc: 0.713141\n",
      "avg val loss: 0.590456 avg val acc: 0.719444\n",
      "-------------------------------\n",
      "Epoch 18\n",
      "avg loss: 0.563626 avg acc: 0.737179\n",
      "avg val loss: 0.644599 avg val acc: 0.630556\n",
      "-------------------------------\n",
      "Epoch 19\n",
      "avg loss: 0.575088 avg acc: 0.717415\n",
      "avg val loss: 0.592843 avg val acc: 0.720833\n",
      "-------------------------------\n",
      "Epoch 20\n",
      "avg loss: 0.565051 avg acc: 0.732372\n",
      "avg val loss: 0.569037 avg val acc: 0.720833\n",
      "-------------------------------\n",
      "Epoch 21\n",
      "avg loss: 0.568030 avg acc: 0.723825\n",
      "avg val loss: 0.602060 avg val acc: 0.697222\n",
      "-------------------------------\n",
      "Epoch 22\n",
      "avg loss: 0.564172 avg acc: 0.725962\n",
      "avg val loss: 0.622319 avg val acc: 0.700000\n",
      "-------------------------------\n",
      "Epoch 23\n",
      "avg loss: 0.575005 avg acc: 0.714744\n",
      "avg val loss: 0.573860 avg val acc: 0.705556\n",
      "-------------------------------\n",
      "Epoch 24\n",
      "avg loss: 0.566816 avg acc: 0.722222\n",
      "avg val loss: 0.579456 avg val acc: 0.720833\n",
      "-------------------------------\n",
      "Epoch 25\n",
      "avg loss: 0.556717 avg acc: 0.727564\n",
      "avg val loss: 0.558200 avg val acc: 0.720833\n",
      "-------------------------------\n",
      "Epoch 26\n",
      "avg loss: 0.560653 avg acc: 0.724893\n",
      "avg val loss: 0.547035 avg val acc: 0.729167\n",
      "-------------------------------\n",
      "Epoch 27\n",
      "avg loss: 0.559414 avg acc: 0.720620\n",
      "avg val loss: 0.565534 avg val acc: 0.705556\n",
      "-------------------------------\n",
      "Epoch 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread SystemMonitor:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/kuko/miniconda3/envs/cv/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/kuko/miniconda3/envs/cv/lib/python3.10/threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/kuko/miniconda3/envs/cv/lib/python3.10/site-packages/wandb/sdk/internal/system/system_monitor.py\", line 118, in _start\n",
      "    asset.start()\n",
      "  File \"/Users/kuko/miniconda3/envs/cv/lib/python3.10/site-packages/wandb/sdk/internal/system/assets/cpu.py\", line 166, in start\n",
      "    self.metrics_monitor.start()\n",
      "  File \"/Users/kuko/miniconda3/envs/cv/lib/python3.10/site-packages/wandb/sdk/internal/system/assets/interfaces.py\", line 168, in start\n",
      "    logger.info(f\"Started {self._process.name}\")\n",
      "AttributeError: 'NoneType' object has no attribute 'name'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg loss: 0.560674 avg acc: 0.729167\n",
      "avg val loss: 0.568614 avg val acc: 0.690278\n",
      "-------------------------------\n",
      "Epoch 29\n",
      "avg loss: 0.564518 avg acc: 0.724893\n",
      "avg val loss: 0.570099 avg val acc: 0.713889\n",
      "-------------------------------\n",
      "Epoch 30\n",
      "avg loss: 0.554932 avg acc: 0.715812\n",
      "avg val loss: 0.619501 avg val acc: 0.677778\n",
      "-------------------------------\n",
      "Epoch 31\n",
      "avg loss: 0.559874 avg acc: 0.727564\n",
      "avg val loss: 0.588160 avg val acc: 0.705556\n",
      "-------------------------------\n",
      "Epoch 32\n",
      "avg loss: 0.557248 avg acc: 0.729167\n",
      "avg val loss: 0.584404 avg val acc: 0.705556\n",
      "-------------------------------\n",
      "Epoch 33\n",
      "avg loss: 0.549117 avg acc: 0.730235\n",
      "avg val loss: 0.575442 avg val acc: 0.712500\n",
      "-------------------------------\n",
      "Epoch 34\n",
      "avg loss: 0.550453 avg acc: 0.726496\n",
      "avg val loss: 0.572022 avg val acc: 0.698611\n",
      "-------------------------------\n",
      "Epoch 35\n",
      "avg loss: 0.552540 avg acc: 0.729167\n",
      "avg val loss: 0.553815 avg val acc: 0.713889\n",
      "-------------------------------\n",
      "Epoch 36\n",
      "avg loss: 0.555316 avg acc: 0.718483\n",
      "avg val loss: 0.553176 avg val acc: 0.700000\n",
      "-------------------------------\n",
      "Epoch 37\n",
      "avg loss: 0.555624 avg acc: 0.733974\n",
      "avg val loss: 0.553062 avg val acc: 0.720833\n",
      "-------------------------------\n",
      "Epoch 38\n",
      "avg loss: 0.550964 avg acc: 0.731838\n",
      "avg val loss: 0.560659 avg val acc: 0.720833\n",
      "-------------------------------\n",
      "Epoch 39\n",
      "avg loss: 0.549366 avg acc: 0.729167\n",
      "avg val loss: 0.564920 avg val acc: 0.727778\n",
      "-------------------------------\n",
      "Epoch 40\n",
      "avg loss: 0.551395 avg acc: 0.728632\n",
      "avg val loss: 0.614157 avg val acc: 0.684722\n",
      "-------------------------------\n",
      "Epoch 41\n",
      "avg loss: 0.550568 avg acc: 0.728098\n",
      "avg val loss: 0.542761 avg val acc: 0.720833\n",
      "-------------------------------\n",
      "Epoch 42\n",
      "avg loss: 0.552187 avg acc: 0.730769\n",
      "avg val loss: 0.562655 avg val acc: 0.698611\n",
      "-------------------------------\n",
      "Epoch 43\n",
      "avg loss: 0.549200 avg acc: 0.731303\n",
      "avg val loss: 0.609528 avg val acc: 0.668056\n",
      "-------------------------------\n",
      "Epoch 44\n",
      "avg loss: 0.546794 avg acc: 0.722756\n",
      "avg val loss: 0.609296 avg val acc: 0.705556\n",
      "-------------------------------\n",
      "Epoch 45\n",
      "avg loss: 0.551829 avg acc: 0.729701\n",
      "avg val loss: 0.539239 avg val acc: 0.743056\n",
      "-------------------------------\n",
      "Epoch 46\n",
      "avg loss: 0.549587 avg acc: 0.725962\n",
      "avg val loss: 0.562767 avg val acc: 0.698611\n",
      "-------------------------------\n",
      "Epoch 47\n",
      "avg loss: 0.546644 avg acc: 0.725427\n",
      "avg val loss: 0.574667 avg val acc: 0.712500\n",
      "-------------------------------\n",
      "Epoch 48\n",
      "avg loss: 0.547742 avg acc: 0.720620\n",
      "avg val loss: 0.545185 avg val acc: 0.736111\n",
      "-------------------------------\n",
      "Epoch 49\n",
      "avg loss: 0.553259 avg acc: 0.724893\n",
      "avg val loss: 0.581693 avg val acc: 0.720833\n",
      "-------------------------------\n",
      "Epoch 50\n",
      "avg loss: 0.547317 avg acc: 0.725427\n",
      "avg val loss: 0.566738 avg val acc: 0.713889\n",
      "-------------------------------\n",
      "Epoch 51\n",
      "avg loss: 0.542674 avg acc: 0.724893\n",
      "avg val loss: 0.560836 avg val acc: 0.705556\n",
      "-------------------------------\n",
      "Epoch 52\n",
      "avg loss: 0.542682 avg acc: 0.727564\n",
      "avg val loss: 0.556292 avg val acc: 0.712500\n",
      "-------------------------------\n",
      "Epoch 53\n",
      "avg loss: 0.550394 avg acc: 0.726496\n",
      "avg val loss: 0.578995 avg val acc: 0.691667\n",
      "-------------------------------\n",
      "Epoch 54\n",
      "avg loss: 0.548917 avg acc: 0.725962\n",
      "avg val loss: 0.567346 avg val acc: 0.705556\n",
      "-------------------------------\n",
      "Epoch 55\n",
      "avg loss: 0.543315 avg acc: 0.725427\n",
      "avg val loss: 0.593032 avg val acc: 0.698611\n",
      "-------------------------------\n",
      "Epoch 56\n",
      "avg loss: 0.544141 avg acc: 0.728098\n",
      "avg val loss: 0.562644 avg val acc: 0.698611\n",
      "-------------------------------\n",
      "Epoch 57\n",
      "avg loss: 0.543822 avg acc: 0.728098\n",
      "avg val loss: 0.594225 avg val acc: 0.676389\n",
      "-------------------------------\n",
      "Epoch 58\n",
      "avg loss: 0.552958 avg acc: 0.738248\n",
      "avg val loss: 0.603927 avg val acc: 0.713889\n",
      "-------------------------------\n",
      "Epoch 59\n",
      "avg loss: 0.546213 avg acc: 0.731838\n",
      "avg val loss: 0.599168 avg val acc: 0.676389\n",
      "-------------------------------\n",
      "Epoch 60\n",
      "avg loss: 0.549410 avg acc: 0.722756\n",
      "avg val loss: 0.583125 avg val acc: 0.698611\n",
      "-------------------------------\n",
      "Epoch 61\n",
      "avg loss: 0.545155 avg acc: 0.727564\n",
      "avg val loss: 0.591852 avg val acc: 0.698611\n",
      "-------------------------------\n",
      "Epoch 62\n",
      "avg loss: 0.541221 avg acc: 0.727030\n",
      "avg val loss: 0.571187 avg val acc: 0.669444\n",
      "-------------------------------\n",
      "Epoch 63\n",
      "avg loss: 0.541193 avg acc: 0.725427\n",
      "avg val loss: 0.569785 avg val acc: 0.698611\n",
      "-------------------------------\n",
      "Epoch 64\n",
      "avg loss: 0.541351 avg acc: 0.734509\n",
      "avg val loss: 0.573272 avg val acc: 0.691667\n",
      "-------------------------------\n",
      "Epoch 65\n",
      "avg loss: 0.545244 avg acc: 0.730235\n",
      "avg val loss: 0.570961 avg val acc: 0.713889\n",
      "-------------------------------\n",
      "Epoch 66\n",
      "avg loss: 0.541533 avg acc: 0.737179\n",
      "avg val loss: 0.554617 avg val acc: 0.727778\n",
      "-------------------------------\n",
      "Epoch 67\n",
      "avg loss: 0.541357 avg acc: 0.733974\n",
      "avg val loss: 0.586287 avg val acc: 0.705556\n",
      "-------------------------------\n",
      "Epoch 68\n",
      "avg loss: 0.541378 avg acc: 0.729701\n",
      "avg val loss: 0.584708 avg val acc: 0.705556\n",
      "-------------------------------\n",
      "Epoch 69\n",
      "avg loss: 0.541945 avg acc: 0.734509\n",
      "avg val loss: 0.575378 avg val acc: 0.698611\n",
      "-------------------------------\n",
      "Epoch 70\n",
      "avg loss: 0.545873 avg acc: 0.738782\n",
      "avg val loss: 0.577145 avg val acc: 0.719444\n",
      "-------------------------------\n",
      "Epoch 71\n",
      "avg loss: 0.547108 avg acc: 0.732372\n",
      "avg val loss: 0.582686 avg val acc: 0.720833\n",
      "-------------------------------\n",
      "Epoch 72\n",
      "avg loss: 0.546847 avg acc: 0.737179\n",
      "avg val loss: 0.577055 avg val acc: 0.698611\n",
      "-------------------------------\n",
      "Epoch 73\n",
      "avg loss: 0.546012 avg acc: 0.730769\n",
      "avg val loss: 0.615656 avg val acc: 0.690278\n",
      "-------------------------------\n",
      "Epoch 74\n",
      "avg loss: 0.542865 avg acc: 0.734509\n",
      "avg val loss: 0.545583 avg val acc: 0.743056\n",
      "-------------------------------\n",
      "Epoch 75\n",
      "avg loss: 0.538248 avg acc: 0.725427\n",
      "avg val loss: 0.604006 avg val acc: 0.705556\n",
      "-------------------------------\n",
      "Epoch 76\n",
      "avg loss: 0.537139 avg acc: 0.728098\n",
      "avg val loss: 0.557530 avg val acc: 0.734722\n",
      "-------------------------------\n",
      "Epoch 77\n",
      "avg loss: 0.537870 avg acc: 0.730235\n",
      "avg val loss: 0.603498 avg val acc: 0.676389\n",
      "-------------------------------\n",
      "Epoch 78\n",
      "avg loss: 0.537616 avg acc: 0.737714\n",
      "avg val loss: 0.548594 avg val acc: 0.727778\n",
      "-------------------------------\n",
      "Epoch 79\n",
      "avg loss: 0.541676 avg acc: 0.726496\n",
      "avg val loss: 0.560370 avg val acc: 0.690278\n",
      "-------------------------------\n",
      "Epoch 80\n",
      "avg loss: 0.539169 avg acc: 0.733440\n",
      "avg val loss: 0.586180 avg val acc: 0.720833\n",
      "-------------------------------\n",
      "Epoch 81\n",
      "avg loss: 0.533916 avg acc: 0.732906\n",
      "avg val loss: 0.546033 avg val acc: 0.713889\n",
      "-------------------------------\n",
      "Epoch 82\n",
      "avg loss: 0.537678 avg acc: 0.732372\n",
      "avg val loss: 0.543799 avg val acc: 0.722222\n",
      "-------------------------------\n",
      "Epoch 83\n",
      "avg loss: 0.535535 avg acc: 0.732906\n",
      "avg val loss: 0.563336 avg val acc: 0.705556\n",
      "-------------------------------\n",
      "Epoch 84\n",
      "avg loss: 0.536331 avg acc: 0.730769\n",
      "avg val loss: 0.541415 avg val acc: 0.727778\n",
      "-------------------------------\n",
      "Epoch 85\n",
      "avg loss: 0.536114 avg acc: 0.729701\n",
      "avg val loss: 0.557512 avg val acc: 0.736111\n",
      "-------------------------------\n",
      "Epoch 86\n",
      "avg loss: 0.538920 avg acc: 0.736111\n",
      "avg val loss: 0.580078 avg val acc: 0.668056\n",
      "-------------------------------\n",
      "Epoch 87\n",
      "avg loss: 0.532955 avg acc: 0.737714\n",
      "avg val loss: 0.570216 avg val acc: 0.720833\n",
      "-------------------------------\n",
      "Epoch 88\n",
      "avg loss: 0.534125 avg acc: 0.735043\n",
      "avg val loss: 0.535624 avg val acc: 0.736111\n",
      "-------------------------------\n",
      "Epoch 89\n",
      "avg loss: 0.531174 avg acc: 0.735043\n",
      "avg val loss: 0.563389 avg val acc: 0.727778\n",
      "-------------------------------\n",
      "Epoch 90\n",
      "avg loss: 0.538844 avg acc: 0.724893\n",
      "avg val loss: 0.540823 avg val acc: 0.729167\n",
      "-------------------------------\n",
      "Epoch 91\n",
      "avg loss: 0.534748 avg acc: 0.730769\n",
      "avg val loss: 0.557202 avg val acc: 0.705556\n",
      "-------------------------------\n",
      "Epoch 92\n",
      "avg loss: 0.532753 avg acc: 0.736645\n",
      "avg val loss: 0.573670 avg val acc: 0.698611\n",
      "-------------------------------\n",
      "Epoch 93\n",
      "avg loss: 0.531985 avg acc: 0.735043\n",
      "avg val loss: 0.577014 avg val acc: 0.683333\n",
      "-------------------------------\n",
      "Epoch 94\n",
      "avg loss: 0.531138 avg acc: 0.735577\n",
      "avg val loss: 0.577693 avg val acc: 0.697222\n",
      "-------------------------------\n",
      "Epoch 95\n",
      "avg loss: 0.539743 avg acc: 0.730235\n",
      "avg val loss: 0.569028 avg val acc: 0.705556\n",
      "-------------------------------\n",
      "Epoch 96\n",
      "avg loss: 0.540920 avg acc: 0.734509\n",
      "avg val loss: 0.596146 avg val acc: 0.690278\n",
      "-------------------------------\n",
      "Epoch 97\n",
      "avg loss: 0.536989 avg acc: 0.733440\n",
      "avg val loss: 0.592064 avg val acc: 0.690278\n",
      "-------------------------------\n",
      "Epoch 98\n",
      "avg loss: 0.538200 avg acc: 0.736111\n",
      "avg val loss: 0.555610 avg val acc: 0.720833\n",
      "-------------------------------\n",
      "Epoch 99\n",
      "avg loss: 0.534931 avg acc: 0.732372\n",
      "avg val loss: 0.557019 avg val acc: 0.727778\n",
      "-------------------------------\n",
      "Epoch 100\n",
      "avg loss: 0.529334 avg acc: 0.737714\n",
      "avg val loss: 0.595878 avg val acc: 0.705556\n",
      "-------------------------------\n",
      "Epoch 101\n",
      "avg loss: 0.538809 avg acc: 0.740919\n",
      "avg val loss: 0.578875 avg val acc: 0.705556\n",
      "-------------------------------\n",
      "Epoch 102\n",
      "avg loss: 0.537452 avg acc: 0.731838\n",
      "avg val loss: 0.543675 avg val acc: 0.720833\n",
      "-------------------------------\n",
      "Epoch 103\n",
      "avg loss: 0.531113 avg acc: 0.743056\n",
      "avg val loss: 0.573563 avg val acc: 0.705556\n",
      "-------------------------------\n",
      "Epoch 104\n",
      "avg loss: 0.527792 avg acc: 0.738782\n",
      "avg val loss: 0.577917 avg val acc: 0.681944\n",
      "-------------------------------\n",
      "Epoch 105\n",
      "avg loss: 0.531514 avg acc: 0.736111\n",
      "avg val loss: 0.551322 avg val acc: 0.720833\n",
      "-------------------------------\n",
      "Epoch 106\n",
      "avg loss: 0.531698 avg acc: 0.734509\n",
      "avg val loss: 0.626224 avg val acc: 0.683333\n",
      "-------------------------------\n",
      "Epoch 107\n",
      "avg loss: 0.531432 avg acc: 0.738782\n",
      "avg val loss: 0.551013 avg val acc: 0.727778\n",
      "-------------------------------\n",
      "Epoch 108\n",
      "avg loss: 0.533361 avg acc: 0.734509\n",
      "avg val loss: 0.530814 avg val acc: 0.750000\n",
      "-------------------------------\n",
      "Epoch 109\n",
      "avg loss: 0.533083 avg acc: 0.741987\n",
      "avg val loss: 0.604885 avg val acc: 0.690278\n",
      "-------------------------------\n",
      "Epoch 110\n",
      "avg loss: 0.534487 avg acc: 0.732906\n",
      "avg val loss: 0.579456 avg val acc: 0.705556\n",
      "-------------------------------\n",
      "Epoch 111\n",
      "avg loss: 0.533657 avg acc: 0.740385\n",
      "avg val loss: 0.550267 avg val acc: 0.720833\n",
      "-------------------------------\n",
      "Epoch 112\n",
      "avg loss: 0.532677 avg acc: 0.738248\n",
      "avg val loss: 0.550122 avg val acc: 0.729167\n",
      "-------------------------------\n",
      "Epoch 113\n",
      "avg loss: 0.528493 avg acc: 0.736111\n",
      "avg val loss: 0.540332 avg val acc: 0.736111\n",
      "-------------------------------\n",
      "Epoch 114\n",
      "avg loss: 0.527342 avg acc: 0.733974\n",
      "avg val loss: 0.549315 avg val acc: 0.729167\n",
      "-------------------------------\n",
      "Epoch 115\n",
      "avg loss: 0.539815 avg acc: 0.738248\n",
      "avg val loss: 0.599699 avg val acc: 0.683333\n",
      "-------------------------------\n",
      "Epoch 116\n",
      "avg loss: 0.539898 avg acc: 0.734509\n",
      "avg val loss: 0.546090 avg val acc: 0.713889\n",
      "-------------------------------\n",
      "Epoch 117\n",
      "avg loss: 0.540486 avg acc: 0.739850\n",
      "avg val loss: 0.596386 avg val acc: 0.712500\n",
      "-------------------------------\n",
      "Epoch 118\n",
      "avg loss: 0.532502 avg acc: 0.740385\n",
      "avg val loss: 0.541105 avg val acc: 0.727778\n",
      "-------------------------------\n",
      "Epoch 119\n",
      "avg loss: 0.530421 avg acc: 0.740385\n",
      "avg val loss: 0.559876 avg val acc: 0.713889\n",
      "-------------------------------\n",
      "Epoch 120\n",
      "avg loss: 0.527366 avg acc: 0.743056\n",
      "avg val loss: 0.558771 avg val acc: 0.713889\n",
      "-------------------------------\n",
      "Epoch 121\n",
      "avg loss: 0.528386 avg acc: 0.740919\n",
      "avg val loss: 0.566783 avg val acc: 0.698611\n",
      "-------------------------------\n",
      "Epoch 122\n",
      "avg loss: 0.528375 avg acc: 0.741987\n",
      "avg val loss: 0.555121 avg val acc: 0.729167\n",
      "-------------------------------\n",
      "Epoch 123\n",
      "avg loss: 0.527112 avg acc: 0.746261\n",
      "avg val loss: 0.561389 avg val acc: 0.698611\n",
      "-------------------------------\n",
      "Epoch 124\n",
      "avg loss: 0.526620 avg acc: 0.748932\n",
      "avg val loss: 0.569515 avg val acc: 0.712500\n",
      "-------------------------------\n",
      "Epoch 125\n",
      "avg loss: 0.521420 avg acc: 0.741987\n",
      "avg val loss: 0.564012 avg val acc: 0.713889\n",
      "-------------------------------\n",
      "Epoch 126\n",
      "avg loss: 0.524528 avg acc: 0.740385\n",
      "avg val loss: 0.561182 avg val acc: 0.712500\n",
      "-------------------------------\n",
      "Epoch 127\n",
      "avg loss: 0.524716 avg acc: 0.743056\n",
      "avg val loss: 0.634475 avg val acc: 0.690278\n",
      "-------------------------------\n",
      "Epoch 128\n",
      "avg loss: 0.526803 avg acc: 0.736645\n",
      "avg val loss: 0.546168 avg val acc: 0.729167\n",
      "-------------------------------\n",
      "Epoch 129\n",
      "avg loss: 0.521839 avg acc: 0.743056\n",
      "avg val loss: 0.549206 avg val acc: 0.705556\n",
      "-------------------------------\n",
      "Epoch 130\n",
      "avg loss: 0.528435 avg acc: 0.739850\n",
      "avg val loss: 0.549116 avg val acc: 0.720833\n",
      "-------------------------------\n",
      "Epoch 131\n",
      "avg loss: 0.520553 avg acc: 0.736645\n",
      "avg val loss: 0.565162 avg val acc: 0.712500\n",
      "-------------------------------\n",
      "Epoch 132\n",
      "avg loss: 0.519045 avg acc: 0.745192\n",
      "avg val loss: 0.552946 avg val acc: 0.729167\n",
      "-------------------------------\n",
      "Epoch 133\n",
      "avg loss: 0.520734 avg acc: 0.740919\n",
      "avg val loss: 0.540680 avg val acc: 0.720833\n",
      "-------------------------------\n",
      "Epoch 134\n",
      "avg loss: 0.519084 avg acc: 0.747329\n",
      "avg val loss: 0.544868 avg val acc: 0.720833\n",
      "-------------------------------\n",
      "Epoch 135\n",
      "avg loss: 0.521454 avg acc: 0.749466\n",
      "avg val loss: 0.549802 avg val acc: 0.729167\n",
      "-------------------------------\n",
      "Epoch 136\n",
      "avg loss: 0.527450 avg acc: 0.732906\n",
      "avg val loss: 0.590022 avg val acc: 0.698611\n",
      "-------------------------------\n",
      "Epoch 137\n",
      "avg loss: 0.524036 avg acc: 0.746261\n",
      "avg val loss: 0.540208 avg val acc: 0.720833\n",
      "-------------------------------\n",
      "Epoch 138\n",
      "avg loss: 0.522787 avg acc: 0.740385\n",
      "avg val loss: 0.596019 avg val acc: 0.683333\n",
      "-------------------------------\n",
      "Epoch 139\n",
      "avg loss: 0.522812 avg acc: 0.739850\n",
      "avg val loss: 0.585609 avg val acc: 0.697222\n",
      "-------------------------------\n",
      "Epoch 140\n",
      "avg loss: 0.518619 avg acc: 0.737179\n",
      "avg val loss: 0.540712 avg val acc: 0.743056\n",
      "-------------------------------\n",
      "Epoch 141\n",
      "avg loss: 0.520093 avg acc: 0.744124\n",
      "avg val loss: 0.609584 avg val acc: 0.690278\n",
      "-------------------------------\n",
      "Epoch 142\n",
      "avg loss: 0.524152 avg acc: 0.745192\n",
      "avg val loss: 0.565845 avg val acc: 0.713889\n",
      "-------------------------------\n",
      "Epoch 143\n",
      "avg loss: 0.528708 avg acc: 0.741987\n",
      "avg val loss: 0.543349 avg val acc: 0.736111\n",
      "-------------------------------\n",
      "Epoch 144\n",
      "avg loss: 0.530338 avg acc: 0.733974\n",
      "avg val loss: 0.573807 avg val acc: 0.719444\n",
      "-------------------------------\n",
      "Epoch 145\n",
      "avg loss: 0.518506 avg acc: 0.747329\n",
      "avg val loss: 0.570972 avg val acc: 0.712500\n",
      "-------------------------------\n",
      "Epoch 146\n",
      "avg loss: 0.516162 avg acc: 0.740919\n",
      "avg val loss: 0.550345 avg val acc: 0.713889\n",
      "-------------------------------\n",
      "Epoch 147\n",
      "avg loss: 0.519593 avg acc: 0.748397\n",
      "avg val loss: 0.566029 avg val acc: 0.713889\n",
      "-------------------------------\n",
      "Epoch 148\n",
      "avg loss: 0.517283 avg acc: 0.741453\n",
      "avg val loss: 0.560218 avg val acc: 0.720833\n",
      "-------------------------------\n",
      "Epoch 149\n",
      "avg loss: 0.514120 avg acc: 0.746795\n",
      "avg val loss: 0.556990 avg val acc: 0.705556\n",
      "-------------------------------\n",
      "Epoch 150\n",
      "avg loss: 0.517672 avg acc: 0.741987\n",
      "avg val loss: 0.531858 avg val acc: 0.722222\n",
      "-------------------------------\n",
      "Epoch 151\n",
      "avg loss: 0.514373 avg acc: 0.747329\n",
      "avg val loss: 0.554948 avg val acc: 0.727778\n",
      "-------------------------------\n",
      "Epoch 152\n",
      "avg loss: 0.515955 avg acc: 0.750534\n",
      "avg val loss: 0.549571 avg val acc: 0.722222\n",
      "-------------------------------\n",
      "Epoch 153\n",
      "avg loss: 0.517666 avg acc: 0.743590\n",
      "avg val loss: 0.572232 avg val acc: 0.727778\n",
      "-------------------------------\n",
      "Epoch 154\n",
      "avg loss: 0.515450 avg acc: 0.747863\n",
      "avg val loss: 0.588593 avg val acc: 0.698611\n",
      "-------------------------------\n",
      "Epoch 155\n",
      "avg loss: 0.514696 avg acc: 0.748932\n",
      "avg val loss: 0.545778 avg val acc: 0.713889\n",
      "-------------------------------\n",
      "Epoch 156\n",
      "avg loss: 0.515932 avg acc: 0.747329\n",
      "avg val loss: 0.542911 avg val acc: 0.706944\n",
      "-------------------------------\n",
      "Epoch 157\n",
      "avg loss: 0.514245 avg acc: 0.740919\n",
      "avg val loss: 0.547868 avg val acc: 0.720833\n",
      "-------------------------------\n",
      "Epoch 158\n",
      "avg loss: 0.516819 avg acc: 0.747329\n",
      "avg val loss: 0.607250 avg val acc: 0.683333\n",
      "-------------------------------\n",
      "Epoch 159\n",
      "avg loss: 0.521073 avg acc: 0.739850\n",
      "avg val loss: 0.567601 avg val acc: 0.719444\n",
      "-------------------------------\n",
      "Epoch 160\n",
      "avg loss: 0.519937 avg acc: 0.744124\n",
      "avg val loss: 0.566281 avg val acc: 0.720833\n",
      "-------------------------------\n",
      "Epoch 161\n",
      "avg loss: 0.509571 avg acc: 0.747863\n",
      "avg val loss: 0.557996 avg val acc: 0.727778\n",
      "-------------------------------\n",
      "Epoch 162\n",
      "avg loss: 0.512683 avg acc: 0.747329\n",
      "avg val loss: 0.554401 avg val acc: 0.727778\n",
      "-------------------------------\n",
      "Epoch 163\n",
      "avg loss: 0.512811 avg acc: 0.750534\n",
      "avg val loss: 0.553460 avg val acc: 0.720833\n",
      "-------------------------------\n",
      "Epoch 164\n",
      "avg loss: 0.510117 avg acc: 0.745192\n",
      "avg val loss: 0.574737 avg val acc: 0.705556\n",
      "-------------------------------\n",
      "Epoch 165\n",
      "avg loss: 0.512469 avg acc: 0.743056\n",
      "avg val loss: 0.563347 avg val acc: 0.736111\n",
      "-------------------------------\n",
      "Epoch 166\n",
      "avg loss: 0.513860 avg acc: 0.739316\n",
      "avg val loss: 0.584653 avg val acc: 0.683333\n",
      "-------------------------------\n",
      "Epoch 167\n",
      "avg loss: 0.514793 avg acc: 0.739316\n",
      "avg val loss: 0.568388 avg val acc: 0.705556\n",
      "-------------------------------\n",
      "Epoch 168\n",
      "avg loss: 0.511053 avg acc: 0.744124\n",
      "avg val loss: 0.554304 avg val acc: 0.727778\n",
      "-------------------------------\n",
      "Epoch 169\n",
      "avg loss: 0.509219 avg acc: 0.755342\n",
      "avg val loss: 0.574749 avg val acc: 0.719444\n",
      "-------------------------------\n",
      "Epoch 170\n",
      "avg loss: 0.510977 avg acc: 0.754808\n",
      "avg val loss: 0.562294 avg val acc: 0.720833\n",
      "-------------------------------\n",
      "Epoch 171\n",
      "avg loss: 0.513417 avg acc: 0.744124\n",
      "avg val loss: 0.645721 avg val acc: 0.698611\n",
      "-------------------------------\n",
      "Epoch 172\n",
      "avg loss: 0.510208 avg acc: 0.750000\n",
      "avg val loss: 0.598747 avg val acc: 0.705556\n",
      "-------------------------------\n",
      "Epoch 173\n",
      "avg loss: 0.514742 avg acc: 0.755342\n",
      "avg val loss: 0.568203 avg val acc: 0.734722\n",
      "-------------------------------\n",
      "Epoch 174\n",
      "avg loss: 0.507795 avg acc: 0.744658\n",
      "avg val loss: 0.557389 avg val acc: 0.736111\n",
      "-------------------------------\n",
      "Epoch 175\n",
      "avg loss: 0.512554 avg acc: 0.746795\n",
      "avg val loss: 0.575204 avg val acc: 0.727778\n",
      "-------------------------------\n",
      "Epoch 176\n",
      "avg loss: 0.504924 avg acc: 0.753205\n",
      "avg val loss: 0.574723 avg val acc: 0.712500\n",
      "-------------------------------\n",
      "Epoch 177\n",
      "avg loss: 0.517283 avg acc: 0.738248\n",
      "avg val loss: 0.630653 avg val acc: 0.690278\n",
      "-------------------------------\n",
      "Epoch 178\n",
      "avg loss: 0.506732 avg acc: 0.749466\n",
      "avg val loss: 0.566649 avg val acc: 0.705556\n",
      "-------------------------------\n",
      "Epoch 179\n",
      "avg loss: 0.511154 avg acc: 0.742521\n",
      "avg val loss: 0.562991 avg val acc: 0.736111\n",
      "-------------------------------\n",
      "Epoch 180\n",
      "avg loss: 0.507299 avg acc: 0.750000\n",
      "avg val loss: 0.624453 avg val acc: 0.697222\n",
      "-------------------------------\n",
      "Epoch 181\n",
      "avg loss: 0.507303 avg acc: 0.747329\n",
      "avg val loss: 0.551612 avg val acc: 0.736111\n",
      "-------------------------------\n",
      "Epoch 182\n",
      "avg loss: 0.501566 avg acc: 0.752137\n",
      "avg val loss: 0.636595 avg val acc: 0.681944\n",
      "-------------------------------\n",
      "Epoch 183\n",
      "avg loss: 0.502591 avg acc: 0.745192\n",
      "avg val loss: 0.551621 avg val acc: 0.736111\n",
      "-------------------------------\n",
      "Epoch 184\n",
      "avg loss: 0.511164 avg acc: 0.740919\n",
      "avg val loss: 0.571786 avg val acc: 0.727778\n",
      "-------------------------------\n",
      "Epoch 185\n",
      "avg loss: 0.502398 avg acc: 0.746795\n",
      "avg val loss: 0.604373 avg val acc: 0.697222\n",
      "-------------------------------\n",
      "Epoch 186\n",
      "avg loss: 0.508305 avg acc: 0.746795\n",
      "avg val loss: 0.606343 avg val acc: 0.697222\n",
      "-------------------------------\n",
      "Epoch 187\n",
      "avg loss: 0.504381 avg acc: 0.754808\n",
      "avg val loss: 0.577144 avg val acc: 0.727778\n",
      "-------------------------------\n",
      "Epoch 188\n",
      "avg loss: 0.506086 avg acc: 0.748397\n",
      "avg val loss: 0.545699 avg val acc: 0.750000\n",
      "-------------------------------\n",
      "Epoch 189\n",
      "avg loss: 0.502667 avg acc: 0.755342\n",
      "avg val loss: 0.569684 avg val acc: 0.727778\n",
      "-------------------------------\n",
      "Epoch 190\n",
      "avg loss: 0.502606 avg acc: 0.756410\n",
      "avg val loss: 0.624447 avg val acc: 0.697222\n",
      "-------------------------------\n",
      "Epoch 191\n",
      "avg loss: 0.511949 avg acc: 0.750000\n",
      "avg val loss: 0.650189 avg val acc: 0.690278\n",
      "-------------------------------\n",
      "Epoch 192\n",
      "avg loss: 0.508080 avg acc: 0.747329\n",
      "avg val loss: 0.587026 avg val acc: 0.712500\n",
      "-------------------------------\n",
      "Epoch 193\n",
      "avg loss: 0.505742 avg acc: 0.747329\n",
      "avg val loss: 0.558583 avg val acc: 0.743056\n",
      "-------------------------------\n",
      "Epoch 194\n",
      "avg loss: 0.502793 avg acc: 0.750000\n",
      "avg val loss: 0.606848 avg val acc: 0.705556\n",
      "-------------------------------\n",
      "Epoch 195\n",
      "avg loss: 0.502045 avg acc: 0.750534\n",
      "avg val loss: 0.559145 avg val acc: 0.720833\n",
      "-------------------------------\n",
      "Epoch 196\n",
      "avg loss: 0.502264 avg acc: 0.749466\n",
      "avg val loss: 0.623707 avg val acc: 0.697222\n",
      "-------------------------------\n",
      "Epoch 197\n",
      "avg loss: 0.508668 avg acc: 0.751068\n",
      "avg val loss: 0.592458 avg val acc: 0.743056\n",
      "-------------------------------\n",
      "Epoch 198\n",
      "avg loss: 0.499985 avg acc: 0.752137\n",
      "avg val loss: 0.591813 avg val acc: 0.726389\n",
      "-------------------------------\n",
      "Epoch 199\n",
      "avg loss: 0.511246 avg acc: 0.753205\n",
      "avg val loss: 0.584263 avg val acc: 0.719444\n",
      "-------------------------------\n",
      "Epoch 200\n",
      "avg loss: 0.502220 avg acc: 0.745192\n",
      "avg val loss: 0.556747 avg val acc: 0.750000\n",
      "-------------------------------\n",
      "Epoch 201\n",
      "avg loss: 0.500889 avg acc: 0.754274\n",
      "avg val loss: 0.607742 avg val acc: 0.705556\n",
      "-------------------------------\n",
      "Epoch 202\n",
      "avg loss: 0.501121 avg acc: 0.756944\n",
      "avg val loss: 0.623179 avg val acc: 0.712500\n",
      "-------------------------------\n",
      "Epoch 203\n",
      "avg loss: 0.498657 avg acc: 0.737714\n",
      "avg val loss: 0.558046 avg val acc: 0.727778\n",
      "-------------------------------\n",
      "Epoch 204\n",
      "avg loss: 0.498855 avg acc: 0.760684\n",
      "avg val loss: 0.595399 avg val acc: 0.719444\n",
      "-------------------------------\n",
      "Epoch 205\n",
      "avg loss: 0.497696 avg acc: 0.750000\n",
      "avg val loss: 0.577410 avg val acc: 0.719444\n",
      "-------------------------------\n",
      "Epoch 206\n",
      "avg loss: 0.500425 avg acc: 0.750000\n",
      "avg val loss: 0.554271 avg val acc: 0.741667\n",
      "-------------------------------\n",
      "Epoch 207\n",
      "avg loss: 0.497465 avg acc: 0.748932\n",
      "avg val loss: 0.559728 avg val acc: 0.743056\n",
      "-------------------------------\n",
      "Epoch 208\n",
      "avg loss: 0.495865 avg acc: 0.753739\n",
      "avg val loss: 0.647773 avg val acc: 0.690278\n",
      "-------------------------------\n",
      "Epoch 209\n",
      "avg loss: 0.494393 avg acc: 0.748932\n",
      "avg val loss: 0.595392 avg val acc: 0.705556\n",
      "-------------------------------\n",
      "Epoch 210\n",
      "avg loss: 0.491804 avg acc: 0.757479\n",
      "avg val loss: 0.602988 avg val acc: 0.734722\n",
      "-------------------------------\n",
      "Epoch 211\n",
      "avg loss: 0.493704 avg acc: 0.748932\n",
      "avg val loss: 0.558490 avg val acc: 0.755556\n",
      "-------------------------------\n",
      "Epoch 212\n",
      "avg loss: 0.498348 avg acc: 0.745726\n",
      "avg val loss: 0.601958 avg val acc: 0.705556\n",
      "-------------------------------\n",
      "Epoch 213\n",
      "avg loss: 0.490409 avg acc: 0.747329\n",
      "avg val loss: 0.618132 avg val acc: 0.712500\n",
      "-------------------------------\n",
      "Epoch 214\n",
      "avg loss: 0.492812 avg acc: 0.754274\n",
      "avg val loss: 0.585862 avg val acc: 0.719444\n",
      "-------------------------------\n",
      "Epoch 215\n",
      "avg loss: 0.492422 avg acc: 0.758013\n",
      "avg val loss: 0.569733 avg val acc: 0.734722\n",
      "-------------------------------\n",
      "Epoch 216\n",
      "avg loss: 0.500210 avg acc: 0.748932\n",
      "avg val loss: 0.543605 avg val acc: 0.736111\n",
      "-------------------------------\n",
      "Epoch 217\n",
      "avg loss: 0.490624 avg acc: 0.762821\n",
      "avg val loss: 0.606255 avg val acc: 0.727778\n",
      "-------------------------------\n",
      "Epoch 218\n",
      "avg loss: 0.491719 avg acc: 0.762286\n",
      "avg val loss: 0.576809 avg val acc: 0.743056\n",
      "-------------------------------\n",
      "Epoch 219\n",
      "avg loss: 0.487492 avg acc: 0.754808\n",
      "avg val loss: 0.585162 avg val acc: 0.698611\n",
      "-------------------------------\n",
      "Epoch 220\n",
      "avg loss: 0.494144 avg acc: 0.756410\n",
      "avg val loss: 0.634934 avg val acc: 0.690278\n",
      "-------------------------------\n",
      "Epoch 221\n",
      "avg loss: 0.489311 avg acc: 0.749466\n",
      "avg val loss: 0.573450 avg val acc: 0.720833\n",
      "-------------------------------\n",
      "Epoch 222\n",
      "avg loss: 0.490706 avg acc: 0.758013\n",
      "avg val loss: 0.631033 avg val acc: 0.719444\n",
      "-------------------------------\n",
      "Epoch 223\n",
      "avg loss: 0.485951 avg acc: 0.753205\n",
      "avg val loss: 0.572262 avg val acc: 0.748611\n",
      "-------------------------------\n",
      "Epoch 224\n",
      "avg loss: 0.490232 avg acc: 0.764423\n",
      "avg val loss: 0.565008 avg val acc: 0.729167\n",
      "-------------------------------\n",
      "Epoch 225\n",
      "avg loss: 0.489933 avg acc: 0.756944\n",
      "avg val loss: 0.584854 avg val acc: 0.698611\n",
      "-------------------------------\n",
      "Epoch 226\n",
      "avg loss: 0.495421 avg acc: 0.750534\n",
      "avg val loss: 0.608368 avg val acc: 0.734722\n",
      "-------------------------------\n",
      "Epoch 227\n",
      "avg loss: 0.490286 avg acc: 0.745192\n",
      "avg val loss: 0.563247 avg val acc: 0.750000\n",
      "-------------------------------\n",
      "Epoch 228\n",
      "avg loss: 0.483346 avg acc: 0.752137\n",
      "avg val loss: 0.607909 avg val acc: 0.705556\n",
      "-------------------------------\n",
      "Epoch 229\n",
      "avg loss: 0.494995 avg acc: 0.745726\n",
      "avg val loss: 0.553319 avg val acc: 0.770833\n",
      "-------------------------------\n",
      "Epoch 230\n",
      "avg loss: 0.489374 avg acc: 0.764423\n",
      "avg val loss: 0.606043 avg val acc: 0.704167\n",
      "-------------------------------\n",
      "Epoch 231\n",
      "avg loss: 0.491757 avg acc: 0.771902\n",
      "avg val loss: 0.560416 avg val acc: 0.736111\n",
      "-------------------------------\n",
      "Epoch 232\n",
      "avg loss: 0.484020 avg acc: 0.749466\n",
      "avg val loss: 0.566995 avg val acc: 0.741667\n",
      "-------------------------------\n",
      "Epoch 233\n",
      "avg loss: 0.482948 avg acc: 0.768162\n",
      "avg val loss: 0.613520 avg val acc: 0.727778\n",
      "-------------------------------\n",
      "Epoch 234\n",
      "avg loss: 0.490976 avg acc: 0.756410\n",
      "avg val loss: 0.575792 avg val acc: 0.750000\n",
      "-------------------------------\n",
      "Epoch 235\n",
      "avg loss: 0.485678 avg acc: 0.756410\n",
      "avg val loss: 0.589463 avg val acc: 0.727778\n",
      "-------------------------------\n",
      "Epoch 236\n",
      "avg loss: 0.490044 avg acc: 0.764957\n",
      "avg val loss: 0.591091 avg val acc: 0.734722\n",
      "-------------------------------\n",
      "Epoch 237\n",
      "avg loss: 0.485087 avg acc: 0.751603\n",
      "avg val loss: 0.586414 avg val acc: 0.727778\n",
      "-------------------------------\n",
      "Epoch 238\n",
      "avg loss: 0.483083 avg acc: 0.755876\n",
      "avg val loss: 0.584610 avg val acc: 0.727778\n",
      "-------------------------------\n",
      "Epoch 239\n",
      "avg loss: 0.480239 avg acc: 0.769765\n",
      "avg val loss: 0.596496 avg val acc: 0.690278\n",
      "-------------------------------\n",
      "Epoch 240\n",
      "avg loss: 0.483240 avg acc: 0.755342\n",
      "avg val loss: 0.592986 avg val acc: 0.719444\n",
      "-------------------------------\n",
      "Epoch 241\n",
      "avg loss: 0.482013 avg acc: 0.764957\n",
      "avg val loss: 0.558324 avg val acc: 0.713889\n",
      "-------------------------------\n",
      "Epoch 242\n",
      "avg loss: 0.496019 avg acc: 0.743056\n",
      "avg val loss: 0.630684 avg val acc: 0.677778\n",
      "-------------------------------\n",
      "Epoch 243\n",
      "avg loss: 0.490896 avg acc: 0.761752\n",
      "avg val loss: 0.590149 avg val acc: 0.720833\n",
      "-------------------------------\n",
      "Epoch 244\n",
      "avg loss: 0.483250 avg acc: 0.761752\n",
      "avg val loss: 0.627233 avg val acc: 0.690278\n",
      "-------------------------------\n",
      "Epoch 245\n",
      "avg loss: 0.480088 avg acc: 0.761218\n",
      "avg val loss: 0.602613 avg val acc: 0.734722\n",
      "-------------------------------\n",
      "Epoch 246\n",
      "avg loss: 0.482997 avg acc: 0.767094\n",
      "avg val loss: 0.576376 avg val acc: 0.727778\n",
      "-------------------------------\n",
      "Epoch 247\n",
      "avg loss: 0.477770 avg acc: 0.761218\n",
      "avg val loss: 0.603279 avg val acc: 0.705556\n",
      "-------------------------------\n",
      "Epoch 248\n",
      "avg loss: 0.477386 avg acc: 0.764423\n",
      "avg val loss: 0.614382 avg val acc: 0.705556\n",
      "-------------------------------\n",
      "Epoch 249\n",
      "avg loss: 0.477575 avg acc: 0.772436\n",
      "avg val loss: 0.588496 avg val acc: 0.697222\n",
      "-------------------------------\n",
      "Epoch 250\n",
      "avg loss: 0.481133 avg acc: 0.755342\n",
      "avg val loss: 0.584189 avg val acc: 0.719444\n",
      "-------------------------------\n",
      "Epoch 251\n",
      "avg loss: 0.475706 avg acc: 0.772436\n",
      "avg val loss: 0.621657 avg val acc: 0.705556\n",
      "-------------------------------\n",
      "Epoch 252\n",
      "avg loss: 0.486941 avg acc: 0.757479\n",
      "avg val loss: 0.567157 avg val acc: 0.743056\n",
      "-------------------------------\n",
      "Epoch 253\n",
      "avg loss: 0.481530 avg acc: 0.763889\n",
      "avg val loss: 0.563144 avg val acc: 0.750000\n",
      "-------------------------------\n",
      "Epoch 254\n",
      "avg loss: 0.475121 avg acc: 0.779380\n",
      "avg val loss: 0.579144 avg val acc: 0.684722\n",
      "-------------------------------\n",
      "Epoch 255\n",
      "avg loss: 0.475653 avg acc: 0.768162\n",
      "avg val loss: 0.595333 avg val acc: 0.734722\n",
      "-------------------------------\n",
      "Epoch 256\n",
      "avg loss: 0.474636 avg acc: 0.763889\n",
      "avg val loss: 0.570498 avg val acc: 0.720833\n",
      "-------------------------------\n",
      "Epoch 257\n",
      "avg loss: 0.471535 avg acc: 0.768697\n",
      "avg val loss: 0.578879 avg val acc: 0.727778\n",
      "-------------------------------\n",
      "Epoch 258\n",
      "avg loss: 0.475094 avg acc: 0.777778\n",
      "avg val loss: 0.589164 avg val acc: 0.727778\n",
      "-------------------------------\n",
      "Epoch 259\n",
      "avg loss: 0.480337 avg acc: 0.763889\n",
      "avg val loss: 0.626290 avg val acc: 0.734722\n",
      "-------------------------------\n",
      "Epoch 260\n",
      "avg loss: 0.481422 avg acc: 0.766560\n",
      "avg val loss: 0.579098 avg val acc: 0.705556\n",
      "-------------------------------\n",
      "Epoch 261\n",
      "avg loss: 0.471502 avg acc: 0.774573\n",
      "avg val loss: 0.649019 avg val acc: 0.669444\n",
      "-------------------------------\n",
      "Epoch 262\n",
      "avg loss: 0.478609 avg acc: 0.762821\n",
      "avg val loss: 0.576392 avg val acc: 0.720833\n",
      "-------------------------------\n",
      "Epoch 263\n",
      "avg loss: 0.477677 avg acc: 0.760684\n",
      "avg val loss: 0.599267 avg val acc: 0.705556\n",
      "-------------------------------\n",
      "Epoch 264\n",
      "avg loss: 0.470563 avg acc: 0.774038\n",
      "avg val loss: 0.608251 avg val acc: 0.705556\n",
      "-------------------------------\n",
      "Epoch 265\n",
      "avg loss: 0.473322 avg acc: 0.759081\n",
      "avg val loss: 0.609298 avg val acc: 0.719444\n",
      "-------------------------------\n",
      "Epoch 266\n",
      "avg loss: 0.475082 avg acc: 0.771902\n",
      "avg val loss: 0.613238 avg val acc: 0.697222\n",
      "-------------------------------\n",
      "Epoch 267\n",
      "avg loss: 0.474567 avg acc: 0.750000\n",
      "avg val loss: 0.653183 avg val acc: 0.719444\n",
      "-------------------------------\n",
      "Epoch 268\n",
      "avg loss: 0.473337 avg acc: 0.773504\n",
      "avg val loss: 0.657662 avg val acc: 0.697222\n",
      "-------------------------------\n",
      "Epoch 269\n",
      "avg loss: 0.476430 avg acc: 0.759081\n",
      "avg val loss: 0.578322 avg val acc: 0.727778\n",
      "-------------------------------\n",
      "Epoch 270\n",
      "avg loss: 0.473944 avg acc: 0.768162\n",
      "avg val loss: 0.618202 avg val acc: 0.712500\n",
      "-------------------------------\n",
      "Epoch 271\n",
      "avg loss: 0.470262 avg acc: 0.779380\n",
      "avg val loss: 0.580068 avg val acc: 0.712500\n",
      "-------------------------------\n",
      "Epoch 272\n",
      "avg loss: 0.473084 avg acc: 0.769231\n",
      "avg val loss: 0.614877 avg val acc: 0.698611\n",
      "-------------------------------\n",
      "Epoch 273\n",
      "avg loss: 0.470379 avg acc: 0.771902\n",
      "avg val loss: 0.665123 avg val acc: 0.697222\n",
      "-------------------------------\n",
      "Epoch 274\n",
      "avg loss: 0.470900 avg acc: 0.779915\n",
      "avg val loss: 0.565807 avg val acc: 0.756944\n",
      "-------------------------------\n",
      "Epoch 275\n",
      "avg loss: 0.468662 avg acc: 0.766560\n",
      "avg val loss: 0.599700 avg val acc: 0.734722\n",
      "-------------------------------\n",
      "Epoch 276\n",
      "avg loss: 0.468659 avg acc: 0.770299\n",
      "avg val loss: 0.592121 avg val acc: 0.712500\n",
      "-------------------------------\n",
      "Epoch 277\n",
      "avg loss: 0.472933 avg acc: 0.762821\n",
      "avg val loss: 0.677872 avg val acc: 0.704167\n",
      "-------------------------------\n",
      "Epoch 278\n",
      "avg loss: 0.468821 avg acc: 0.772970\n",
      "avg val loss: 0.613731 avg val acc: 0.712500\n",
      "-------------------------------\n",
      "Epoch 279\n",
      "avg loss: 0.467417 avg acc: 0.781517\n",
      "avg val loss: 0.581270 avg val acc: 0.726389\n",
      "-------------------------------\n",
      "Epoch 280\n",
      "avg loss: 0.472694 avg acc: 0.778846\n",
      "avg val loss: 0.626564 avg val acc: 0.712500\n",
      "-------------------------------\n",
      "Epoch 281\n",
      "avg loss: 0.463865 avg acc: 0.778312\n",
      "avg val loss: 0.584220 avg val acc: 0.777778\n",
      "-------------------------------\n",
      "Epoch 282\n",
      "avg loss: 0.463679 avg acc: 0.785256\n",
      "avg val loss: 0.632276 avg val acc: 0.683333\n",
      "-------------------------------\n",
      "Epoch 283\n",
      "avg loss: 0.466254 avg acc: 0.776175\n",
      "avg val loss: 0.634098 avg val acc: 0.712500\n",
      "-------------------------------\n",
      "Epoch 284\n",
      "avg loss: 0.471213 avg acc: 0.770833\n",
      "avg val loss: 0.598746 avg val acc: 0.705556\n",
      "-------------------------------\n",
      "Epoch 285\n",
      "avg loss: 0.460130 avg acc: 0.777244\n",
      "avg val loss: 0.633728 avg val acc: 0.726389\n",
      "-------------------------------\n",
      "Epoch 286\n",
      "avg loss: 0.463741 avg acc: 0.771368\n",
      "avg val loss: 0.607612 avg val acc: 0.727778\n",
      "-------------------------------\n",
      "Epoch 287\n",
      "avg loss: 0.469678 avg acc: 0.767094\n",
      "avg val loss: 0.709709 avg val acc: 0.681944\n",
      "-------------------------------\n",
      "Epoch 288\n",
      "avg loss: 0.469047 avg acc: 0.774573\n",
      "avg val loss: 0.607418 avg val acc: 0.727778\n",
      "-------------------------------\n",
      "Epoch 289\n",
      "avg loss: 0.463127 avg acc: 0.779915\n",
      "avg val loss: 0.624629 avg val acc: 0.711111\n",
      "-------------------------------\n",
      "Epoch 290\n",
      "avg loss: 0.475633 avg acc: 0.780449\n",
      "avg val loss: 0.619519 avg val acc: 0.705556\n",
      "-------------------------------\n",
      "Epoch 291\n",
      "avg loss: 0.462947 avg acc: 0.772970\n",
      "avg val loss: 0.628670 avg val acc: 0.719444\n",
      "-------------------------------\n",
      "Epoch 292\n",
      "avg loss: 0.464245 avg acc: 0.765491\n",
      "avg val loss: 0.578390 avg val acc: 0.741667\n",
      "-------------------------------\n",
      "Epoch 293\n",
      "avg loss: 0.462907 avg acc: 0.779380\n",
      "avg val loss: 0.606646 avg val acc: 0.712500\n",
      "-------------------------------\n",
      "Epoch 294\n",
      "avg loss: 0.470333 avg acc: 0.762286\n",
      "avg val loss: 0.591482 avg val acc: 0.727778\n",
      "-------------------------------\n",
      "Epoch 295\n",
      "avg loss: 0.464475 avg acc: 0.769765\n",
      "avg val loss: 0.599326 avg val acc: 0.684722\n",
      "-------------------------------\n",
      "Epoch 296\n",
      "avg loss: 0.466701 avg acc: 0.784188\n",
      "avg val loss: 0.548751 avg val acc: 0.736111\n",
      "-------------------------------\n",
      "Epoch 297\n",
      "avg loss: 0.464803 avg acc: 0.778846\n",
      "avg val loss: 0.604329 avg val acc: 0.748611\n",
      "-------------------------------\n",
      "Epoch 298\n",
      "avg loss: 0.458424 avg acc: 0.780449\n",
      "avg val loss: 0.650271 avg val acc: 0.734722\n",
      "-------------------------------\n",
      "Epoch 299\n",
      "avg loss: 0.466327 avg acc: 0.775641\n",
      "avg val loss: 0.597374 avg val acc: 0.713889\n",
      "-------------------------------\n",
      "Epoch 300\n",
      "avg loss: 0.455282 avg acc: 0.782051\n",
      "avg val loss: 0.645274 avg val acc: 0.711111\n",
      "-------------------------------\n",
      "Epoch 301\n",
      "avg loss: 0.468933 avg acc: 0.778312\n",
      "avg val loss: 0.568568 avg val acc: 0.708333\n",
      "-------------------------------\n",
      "Epoch 302\n",
      "avg loss: 0.465199 avg acc: 0.774038\n",
      "avg val loss: 0.661875 avg val acc: 0.727778\n",
      "-------------------------------\n",
      "Epoch 303\n",
      "avg loss: 0.462400 avg acc: 0.775641\n",
      "avg val loss: 0.622224 avg val acc: 0.740278\n",
      "-------------------------------\n",
      "Epoch 304\n",
      "avg loss: 0.469664 avg acc: 0.772970\n",
      "avg val loss: 0.621402 avg val acc: 0.722222\n",
      "-------------------------------\n",
      "Epoch 305\n",
      "avg loss: 0.469221 avg acc: 0.757479\n",
      "avg val loss: 0.660570 avg val acc: 0.697222\n",
      "-------------------------------\n",
      "Epoch 306\n",
      "avg loss: 0.454456 avg acc: 0.787927\n",
      "avg val loss: 0.615945 avg val acc: 0.720833\n",
      "-------------------------------\n",
      "Epoch 307\n",
      "avg loss: 0.455895 avg acc: 0.784722\n",
      "avg val loss: 0.613820 avg val acc: 0.727778\n",
      "-------------------------------\n",
      "Epoch 308\n",
      "avg loss: 0.457755 avg acc: 0.786325\n",
      "avg val loss: 0.593995 avg val acc: 0.720833\n",
      "-------------------------------\n",
      "Epoch 309\n",
      "avg loss: 0.456712 avg acc: 0.773504\n",
      "avg val loss: 0.659788 avg val acc: 0.720833\n",
      "-------------------------------\n",
      "Epoch 310\n",
      "avg loss: 0.456107 avg acc: 0.781517\n",
      "avg val loss: 0.620059 avg val acc: 0.705556\n",
      "-------------------------------\n",
      "Epoch 311\n",
      "avg loss: 0.457708 avg acc: 0.778846\n",
      "avg val loss: 0.629303 avg val acc: 0.712500\n",
      "-------------------------------\n",
      "Epoch 312\n",
      "avg loss: 0.466059 avg acc: 0.771368\n",
      "avg val loss: 0.618955 avg val acc: 0.698611\n",
      "-------------------------------\n",
      "Epoch 313\n",
      "avg loss: 0.467725 avg acc: 0.778312\n",
      "avg val loss: 0.677540 avg val acc: 0.690278\n",
      "-------------------------------\n",
      "Epoch 314\n",
      "avg loss: 0.451297 avg acc: 0.790064\n",
      "avg val loss: 0.556585 avg val acc: 0.770833\n",
      "-------------------------------\n",
      "Epoch 315\n",
      "avg loss: 0.455943 avg acc: 0.784722\n",
      "avg val loss: 0.580421 avg val acc: 0.736111\n",
      "-------------------------------\n",
      "Epoch 316\n",
      "avg loss: 0.451445 avg acc: 0.786325\n",
      "avg val loss: 0.651708 avg val acc: 0.675000\n",
      "-------------------------------\n",
      "Epoch 317\n",
      "avg loss: 0.464715 avg acc: 0.762286\n",
      "avg val loss: 0.659597 avg val acc: 0.676389\n",
      "-------------------------------\n",
      "Epoch 318\n",
      "avg loss: 0.459389 avg acc: 0.781517\n",
      "avg val loss: 0.597693 avg val acc: 0.736111\n",
      "-------------------------------\n",
      "Epoch 319\n",
      "avg loss: 0.452651 avg acc: 0.788462\n",
      "avg val loss: 0.653290 avg val acc: 0.705556\n",
      "-------------------------------\n",
      "Epoch 320\n",
      "avg loss: 0.452838 avg acc: 0.790064\n",
      "avg val loss: 0.645268 avg val acc: 0.734722\n",
      "-------------------------------\n",
      "Epoch 321\n",
      "avg loss: 0.447400 avg acc: 0.786859\n",
      "avg val loss: 0.613851 avg val acc: 0.748611\n",
      "-------------------------------\n",
      "Epoch 322\n",
      "avg loss: 0.458217 avg acc: 0.774573\n",
      "avg val loss: 0.680062 avg val acc: 0.711111\n",
      "-------------------------------\n",
      "Epoch 323\n",
      "avg loss: 0.449993 avg acc: 0.783120\n",
      "avg val loss: 0.624083 avg val acc: 0.720833\n",
      "-------------------------------\n",
      "Epoch 324\n",
      "avg loss: 0.463046 avg acc: 0.761218\n",
      "avg val loss: 0.624243 avg val acc: 0.719444\n",
      "-------------------------------\n",
      "Epoch 325\n",
      "avg loss: 0.451252 avg acc: 0.793269\n",
      "avg val loss: 0.652194 avg val acc: 0.690278\n",
      "-------------------------------\n",
      "Epoch 326\n",
      "avg loss: 0.455091 avg acc: 0.782051\n",
      "avg val loss: 0.637298 avg val acc: 0.713889\n",
      "-------------------------------\n",
      "Epoch 327\n",
      "avg loss: 0.458474 avg acc: 0.772970\n",
      "avg val loss: 0.700714 avg val acc: 0.683333\n",
      "-------------------------------\n",
      "Epoch 328\n",
      "avg loss: 0.453807 avg acc: 0.782051\n",
      "avg val loss: 0.589410 avg val acc: 0.736111\n",
      "-------------------------------\n",
      "Epoch 329\n",
      "avg loss: 0.452038 avg acc: 0.791132\n",
      "avg val loss: 0.604939 avg val acc: 0.743056\n",
      "-------------------------------\n",
      "Epoch 330\n",
      "avg loss: 0.455885 avg acc: 0.782585\n",
      "avg val loss: 0.622142 avg val acc: 0.734722\n",
      "-------------------------------\n",
      "Epoch 331\n",
      "avg loss: 0.457941 avg acc: 0.785256\n",
      "avg val loss: 0.616746 avg val acc: 0.727778\n",
      "-------------------------------\n",
      "Epoch 332\n",
      "avg loss: 0.449634 avg acc: 0.787393\n",
      "avg val loss: 0.619038 avg val acc: 0.727778\n",
      "-------------------------------\n",
      "Epoch 333\n",
      "avg loss: 0.451825 avg acc: 0.783120\n",
      "avg val loss: 0.660133 avg val acc: 0.675000\n",
      "-------------------------------\n",
      "Epoch 334\n",
      "avg loss: 0.445855 avg acc: 0.792201\n",
      "avg val loss: 0.588606 avg val acc: 0.743056\n",
      "-------------------------------\n",
      "Epoch 335\n",
      "avg loss: 0.452701 avg acc: 0.783654\n",
      "avg val loss: 0.697609 avg val acc: 0.705556\n",
      "-------------------------------\n",
      "Epoch 336\n",
      "avg loss: 0.451286 avg acc: 0.782051\n",
      "avg val loss: 0.611050 avg val acc: 0.697222\n",
      "-------------------------------\n",
      "Epoch 337\n",
      "avg loss: 0.456976 avg acc: 0.779915\n",
      "avg val loss: 0.666936 avg val acc: 0.705556\n",
      "-------------------------------\n",
      "Epoch 338\n",
      "avg loss: 0.458244 avg acc: 0.770833\n",
      "avg val loss: 0.648691 avg val acc: 0.690278\n",
      "-------------------------------\n",
      "Epoch 339\n",
      "avg loss: 0.453706 avg acc: 0.776709\n",
      "avg val loss: 0.605614 avg val acc: 0.777778\n",
      "-------------------------------\n",
      "Epoch 340\n",
      "avg loss: 0.455239 avg acc: 0.775107\n",
      "avg val loss: 0.583727 avg val acc: 0.722222\n",
      "-------------------------------\n",
      "Epoch 341\n",
      "avg loss: 0.448199 avg acc: 0.782585\n",
      "avg val loss: 0.645786 avg val acc: 0.683333\n",
      "-------------------------------\n",
      "Epoch 342\n",
      "avg loss: 0.453342 avg acc: 0.787927\n",
      "avg val loss: 0.618209 avg val acc: 0.727778\n",
      "-------------------------------\n",
      "Epoch 343\n",
      "avg loss: 0.438665 avg acc: 0.787927\n",
      "avg val loss: 0.607018 avg val acc: 0.719444\n",
      "-------------------------------\n",
      "Epoch 344\n",
      "avg loss: 0.446168 avg acc: 0.791667\n",
      "avg val loss: 0.611291 avg val acc: 0.713889\n",
      "-------------------------------\n",
      "Epoch 345\n",
      "avg loss: 0.448839 avg acc: 0.777778\n",
      "avg val loss: 0.662918 avg val acc: 0.705556\n",
      "-------------------------------\n",
      "Epoch 346\n",
      "avg loss: 0.444472 avg acc: 0.776709\n",
      "avg val loss: 0.621474 avg val acc: 0.713889\n",
      "-------------------------------\n",
      "Epoch 347\n",
      "avg loss: 0.446980 avg acc: 0.790064\n",
      "avg val loss: 0.697044 avg val acc: 0.688889\n",
      "-------------------------------\n",
      "Epoch 348\n",
      "avg loss: 0.451070 avg acc: 0.787927\n",
      "avg val loss: 0.616117 avg val acc: 0.736111\n",
      "-------------------------------\n",
      "Epoch 349\n",
      "avg loss: 0.442553 avg acc: 0.783120\n",
      "avg val loss: 0.635268 avg val acc: 0.736111\n",
      "-------------------------------\n",
      "Epoch 350\n",
      "avg loss: 0.449348 avg acc: 0.773504\n",
      "avg val loss: 0.635261 avg val acc: 0.756944\n",
      "-------------------------------\n",
      "Epoch 351\n",
      "avg loss: 0.445908 avg acc: 0.795406\n",
      "avg val loss: 0.652208 avg val acc: 0.720833\n",
      "-------------------------------\n",
      "Epoch 352\n",
      "avg loss: 0.451926 avg acc: 0.774573\n",
      "avg val loss: 0.632037 avg val acc: 0.720833\n",
      "-------------------------------\n",
      "Epoch 353\n",
      "avg loss: 0.447861 avg acc: 0.786325\n",
      "avg val loss: 0.679284 avg val acc: 0.720833\n",
      "-------------------------------\n",
      "Epoch 354\n",
      "avg loss: 0.446594 avg acc: 0.775641\n",
      "avg val loss: 0.680279 avg val acc: 0.720833\n",
      "-------------------------------\n",
      "Epoch 355\n",
      "avg loss: 0.444444 avg acc: 0.794338\n",
      "avg val loss: 0.642371 avg val acc: 0.705556\n",
      "-------------------------------\n",
      "Epoch 356\n",
      "avg loss: 0.439451 avg acc: 0.787393\n",
      "avg val loss: 0.769568 avg val acc: 0.683333\n",
      "-------------------------------\n",
      "Epoch 357\n",
      "avg loss: 0.448191 avg acc: 0.781517\n",
      "avg val loss: 0.674436 avg val acc: 0.681944\n",
      "-------------------------------\n",
      "Epoch 358\n",
      "avg loss: 0.444512 avg acc: 0.796474\n",
      "avg val loss: 0.635024 avg val acc: 0.741667\n",
      "-------------------------------\n",
      "Epoch 359\n",
      "avg loss: 0.445380 avg acc: 0.792735\n",
      "avg val loss: 0.674644 avg val acc: 0.697222\n",
      "-------------------------------\n",
      "Epoch 360\n",
      "avg loss: 0.444658 avg acc: 0.790064\n",
      "avg val loss: 0.649971 avg val acc: 0.706944\n",
      "-------------------------------\n",
      "Epoch 361\n",
      "avg loss: 0.442822 avg acc: 0.794872\n",
      "avg val loss: 0.589652 avg val acc: 0.727778\n",
      "-------------------------------\n",
      "Epoch 362\n",
      "avg loss: 0.444476 avg acc: 0.793269\n",
      "avg val loss: 0.637571 avg val acc: 0.711111\n",
      "-------------------------------\n",
      "Epoch 363\n",
      "avg loss: 0.444787 avg acc: 0.787927\n",
      "avg val loss: 0.629698 avg val acc: 0.722222\n",
      "-------------------------------\n",
      "Epoch 364\n",
      "avg loss: 0.448144 avg acc: 0.791132\n",
      "avg val loss: 0.641587 avg val acc: 0.736111\n",
      "-------------------------------\n",
      "Epoch 365\n",
      "avg loss: 0.440576 avg acc: 0.787393\n",
      "avg val loss: 0.593713 avg val acc: 0.750000\n",
      "-------------------------------\n",
      "Epoch 366\n",
      "avg loss: 0.431654 avg acc: 0.800214\n",
      "avg val loss: 0.627812 avg val acc: 0.691667\n",
      "-------------------------------\n",
      "Epoch 367\n",
      "avg loss: 0.446953 avg acc: 0.779915\n",
      "avg val loss: 0.614831 avg val acc: 0.743056\n",
      "-------------------------------\n",
      "Epoch 368\n",
      "avg loss: 0.439850 avg acc: 0.788462\n",
      "avg val loss: 0.658132 avg val acc: 0.647222\n",
      "-------------------------------\n",
      "Epoch 369\n",
      "avg loss: 0.439804 avg acc: 0.776709\n",
      "avg val loss: 0.625933 avg val acc: 0.719444\n",
      "-------------------------------\n",
      "Epoch 370\n",
      "avg loss: 0.442122 avg acc: 0.798077\n",
      "avg val loss: 0.613488 avg val acc: 0.750000\n",
      "-------------------------------\n",
      "Epoch 371\n",
      "avg loss: 0.440548 avg acc: 0.800748\n",
      "avg val loss: 0.582898 avg val acc: 0.743056\n",
      "-------------------------------\n",
      "Epoch 372\n",
      "avg loss: 0.444964 avg acc: 0.784188\n",
      "avg val loss: 0.642393 avg val acc: 0.686111\n",
      "-------------------------------\n",
      "Epoch 373\n",
      "avg loss: 0.435197 avg acc: 0.797543\n",
      "avg val loss: 0.660161 avg val acc: 0.756944\n",
      "-------------------------------\n",
      "Epoch 374\n",
      "avg loss: 0.444409 avg acc: 0.785256\n",
      "avg val loss: 0.661091 avg val acc: 0.706944\n",
      "-------------------------------\n",
      "Epoch 375\n",
      "avg loss: 0.431736 avg acc: 0.798611\n",
      "avg val loss: 0.639222 avg val acc: 0.734722\n",
      "-------------------------------\n",
      "Epoch 376\n",
      "avg loss: 0.431877 avg acc: 0.795940\n",
      "avg val loss: 0.649014 avg val acc: 0.713889\n",
      "-------------------------------\n",
      "Epoch 377\n",
      "avg loss: 0.437451 avg acc: 0.788462\n",
      "avg val loss: 0.595873 avg val acc: 0.743056\n",
      "-------------------------------\n",
      "Epoch 378\n",
      "avg loss: 0.436118 avg acc: 0.790598\n",
      "avg val loss: 0.642512 avg val acc: 0.734722\n",
      "-------------------------------\n",
      "Epoch 379\n",
      "avg loss: 0.436692 avg acc: 0.785256\n",
      "avg val loss: 0.594577 avg val acc: 0.756944\n",
      "-------------------------------\n",
      "Epoch 380\n",
      "avg loss: 0.433405 avg acc: 0.788462\n",
      "avg val loss: 0.679359 avg val acc: 0.683333\n",
      "-------------------------------\n",
      "Epoch 381\n",
      "avg loss: 0.431011 avg acc: 0.791132\n",
      "avg val loss: 0.640268 avg val acc: 0.698611\n",
      "-------------------------------\n",
      "Epoch 382\n",
      "avg loss: 0.426182 avg acc: 0.795940\n",
      "avg val loss: 0.604581 avg val acc: 0.722222\n",
      "-------------------------------\n",
      "Epoch 383\n",
      "avg loss: 0.427815 avg acc: 0.803953\n",
      "avg val loss: 0.643515 avg val acc: 0.720833\n",
      "-------------------------------\n",
      "Epoch 384\n",
      "avg loss: 0.441599 avg acc: 0.791132\n",
      "avg val loss: 0.632856 avg val acc: 0.715278\n",
      "-------------------------------\n",
      "Epoch 385\n",
      "avg loss: 0.442326 avg acc: 0.783654\n",
      "avg val loss: 0.639079 avg val acc: 0.693056\n",
      "-------------------------------\n",
      "Epoch 386\n",
      "avg loss: 0.437014 avg acc: 0.783120\n",
      "avg val loss: 0.624471 avg val acc: 0.691667\n",
      "-------------------------------\n",
      "Epoch 387\n",
      "avg loss: 0.433507 avg acc: 0.795406\n",
      "avg val loss: 0.719073 avg val acc: 0.734722\n",
      "-------------------------------\n",
      "Epoch 388\n",
      "avg loss: 0.435173 avg acc: 0.792201\n",
      "avg val loss: 0.622095 avg val acc: 0.705556\n",
      "-------------------------------\n",
      "Epoch 389\n",
      "avg loss: 0.429145 avg acc: 0.791132\n",
      "avg val loss: 0.617151 avg val acc: 0.736111\n",
      "-------------------------------\n",
      "Epoch 390\n",
      "avg loss: 0.426059 avg acc: 0.798077\n",
      "avg val loss: 0.697000 avg val acc: 0.683333\n",
      "-------------------------------\n",
      "Epoch 391\n",
      "avg loss: 0.430274 avg acc: 0.789530\n",
      "avg val loss: 0.659293 avg val acc: 0.705556\n",
      "-------------------------------\n",
      "Epoch 392\n",
      "avg loss: 0.439627 avg acc: 0.783120\n",
      "avg val loss: 0.682396 avg val acc: 0.697222\n",
      "-------------------------------\n",
      "Epoch 393\n",
      "avg loss: 0.428230 avg acc: 0.797009\n",
      "avg val loss: 0.620833 avg val acc: 0.727778\n",
      "-------------------------------\n",
      "Epoch 394\n",
      "avg loss: 0.426336 avg acc: 0.787927\n",
      "avg val loss: 0.664852 avg val acc: 0.690278\n",
      "-------------------------------\n",
      "Epoch 395\n",
      "avg loss: 0.423497 avg acc: 0.795940\n",
      "avg val loss: 0.708990 avg val acc: 0.712500\n",
      "-------------------------------\n",
      "Epoch 396\n",
      "avg loss: 0.432911 avg acc: 0.791667\n",
      "avg val loss: 0.627605 avg val acc: 0.734722\n",
      "-------------------------------\n",
      "Epoch 397\n",
      "avg loss: 0.434261 avg acc: 0.802350\n",
      "avg val loss: 0.649536 avg val acc: 0.727778\n",
      "-------------------------------\n",
      "Epoch 398\n",
      "avg loss: 0.437600 avg acc: 0.784188\n",
      "avg val loss: 0.629294 avg val acc: 0.706944\n",
      "-------------------------------\n",
      "Epoch 399\n",
      "avg loss: 0.432685 avg acc: 0.795940\n",
      "avg val loss: 0.741917 avg val acc: 0.681944\n",
      "-------------------------------\n",
      "Epoch 400\n",
      "avg loss: 0.426335 avg acc: 0.792735\n",
      "avg val loss: 0.695699 avg val acc: 0.712500\n",
      "-------------------------------\n",
      "Epoch 401\n",
      "avg loss: 0.427977 avg acc: 0.797009\n",
      "avg val loss: 0.680331 avg val acc: 0.705556\n",
      "-------------------------------\n",
      "Epoch 402\n",
      "avg loss: 0.427752 avg acc: 0.794872\n",
      "avg val loss: 0.650674 avg val acc: 0.700000\n",
      "-------------------------------\n",
      "Epoch 403\n",
      "avg loss: 0.431146 avg acc: 0.800214\n",
      "avg val loss: 0.627969 avg val acc: 0.720833\n",
      "-------------------------------\n",
      "Epoch 404\n",
      "avg loss: 0.435707 avg acc: 0.787393\n",
      "avg val loss: 0.638972 avg val acc: 0.700000\n",
      "-------------------------------\n",
      "Epoch 405\n",
      "avg loss: 0.431670 avg acc: 0.793803\n",
      "avg val loss: 0.632010 avg val acc: 0.763889\n",
      "-------------------------------\n",
      "Epoch 406\n",
      "avg loss: 0.425742 avg acc: 0.802350\n",
      "avg val loss: 0.622274 avg val acc: 0.720833\n",
      "-------------------------------\n",
      "Epoch 407\n",
      "avg loss: 0.436526 avg acc: 0.777244\n",
      "avg val loss: 0.652861 avg val acc: 0.722222\n",
      "-------------------------------\n",
      "Epoch 408\n",
      "avg loss: 0.429574 avg acc: 0.803419\n",
      "avg val loss: 0.666689 avg val acc: 0.747222\n",
      "-------------------------------\n",
      "Epoch 409\n",
      "avg loss: 0.426482 avg acc: 0.798077\n",
      "avg val loss: 0.603421 avg val acc: 0.729167\n",
      "-------------------------------\n",
      "Epoch 410\n",
      "avg loss: 0.426600 avg acc: 0.786859\n",
      "avg val loss: 0.660194 avg val acc: 0.713889\n",
      "-------------------------------\n",
      "Epoch 411\n",
      "avg loss: 0.432038 avg acc: 0.787393\n",
      "avg val loss: 0.707308 avg val acc: 0.655556\n",
      "-------------------------------\n",
      "Epoch 412\n",
      "avg loss: 0.427838 avg acc: 0.793269\n",
      "avg val loss: 0.594593 avg val acc: 0.736111\n",
      "-------------------------------\n",
      "Epoch 413\n",
      "avg loss: 0.425999 avg acc: 0.791132\n",
      "avg val loss: 0.634573 avg val acc: 0.712500\n",
      "-------------------------------\n",
      "Epoch 414\n",
      "avg loss: 0.431652 avg acc: 0.791667\n",
      "avg val loss: 0.659738 avg val acc: 0.720833\n",
      "-------------------------------\n",
      "Epoch 415\n",
      "avg loss: 0.431670 avg acc: 0.800214\n",
      "avg val loss: 0.642322 avg val acc: 0.727778\n",
      "-------------------------------\n",
      "Epoch 416\n",
      "avg loss: 0.418180 avg acc: 0.804487\n",
      "avg val loss: 0.663574 avg val acc: 0.729167\n",
      "-------------------------------\n",
      "Epoch 417\n",
      "avg loss: 0.425623 avg acc: 0.798611\n",
      "avg val loss: 0.602312 avg val acc: 0.701389\n",
      "-------------------------------\n",
      "Epoch 418\n",
      "avg loss: 0.438980 avg acc: 0.791667\n",
      "avg val loss: 0.676590 avg val acc: 0.698611\n",
      "-------------------------------\n",
      "Epoch 419\n",
      "avg loss: 0.420331 avg acc: 0.805556\n",
      "avg val loss: 0.618659 avg val acc: 0.763889\n",
      "-------------------------------\n",
      "Epoch 420\n",
      "avg loss: 0.426021 avg acc: 0.795940\n",
      "avg val loss: 0.671803 avg val acc: 0.712500\n",
      "-------------------------------\n",
      "Epoch 421\n",
      "avg loss: 0.425766 avg acc: 0.793269\n",
      "avg val loss: 0.625429 avg val acc: 0.755556\n",
      "-------------------------------\n",
      "Epoch 422\n",
      "avg loss: 0.425527 avg acc: 0.800214\n",
      "avg val loss: 0.743582 avg val acc: 0.734722\n",
      "-------------------------------\n",
      "Epoch 423\n",
      "avg loss: 0.427597 avg acc: 0.791132\n",
      "avg val loss: 0.674567 avg val acc: 0.720833\n",
      "-------------------------------\n",
      "Epoch 424\n",
      "avg loss: 0.425081 avg acc: 0.795940\n",
      "avg val loss: 0.620894 avg val acc: 0.670833\n",
      "-------------------------------\n",
      "Epoch 425\n",
      "avg loss: 0.416885 avg acc: 0.797009\n",
      "avg val loss: 0.657537 avg val acc: 0.715278\n",
      "-------------------------------\n",
      "Epoch 426\n",
      "avg loss: 0.427638 avg acc: 0.788996\n",
      "avg val loss: 0.619381 avg val acc: 0.734722\n",
      "-------------------------------\n",
      "Epoch 427\n",
      "avg loss: 0.420668 avg acc: 0.797009\n",
      "avg val loss: 0.639222 avg val acc: 0.713889\n",
      "-------------------------------\n",
      "Epoch 428\n",
      "avg loss: 0.418244 avg acc: 0.795940\n",
      "avg val loss: 0.611319 avg val acc: 0.743056\n",
      "-------------------------------\n",
      "Epoch 429\n",
      "avg loss: 0.419299 avg acc: 0.797009\n",
      "avg val loss: 0.677177 avg val acc: 0.734722\n",
      "-------------------------------\n",
      "Epoch 430\n",
      "avg loss: 0.421179 avg acc: 0.800214\n",
      "avg val loss: 0.634890 avg val acc: 0.727778\n",
      "-------------------------------\n",
      "Epoch 431\n",
      "avg loss: 0.426793 avg acc: 0.798611\n",
      "avg val loss: 0.680718 avg val acc: 0.704167\n",
      "-------------------------------\n",
      "Epoch 432\n",
      "avg loss: 0.424506 avg acc: 0.796474\n",
      "avg val loss: 0.615587 avg val acc: 0.720833\n",
      "-------------------------------\n",
      "Epoch 433\n",
      "avg loss: 0.410378 avg acc: 0.808761\n",
      "avg val loss: 0.628859 avg val acc: 0.763889\n",
      "-------------------------------\n",
      "Epoch 434\n",
      "avg loss: 0.416551 avg acc: 0.791132\n",
      "avg val loss: 0.768000 avg val acc: 0.688889\n",
      "-------------------------------\n",
      "Epoch 435\n",
      "avg loss: 0.416340 avg acc: 0.792735\n",
      "avg val loss: 0.684666 avg val acc: 0.683333\n",
      "-------------------------------\n",
      "Epoch 436\n",
      "avg loss: 0.419797 avg acc: 0.801282\n",
      "avg val loss: 0.672109 avg val acc: 0.705556\n",
      "-------------------------------\n",
      "Epoch 437\n",
      "avg loss: 0.415964 avg acc: 0.809295\n",
      "avg val loss: 0.655970 avg val acc: 0.697222\n",
      "-------------------------------\n",
      "Epoch 438\n",
      "avg loss: 0.413315 avg acc: 0.800748\n",
      "avg val loss: 0.655673 avg val acc: 0.698611\n",
      "-------------------------------\n",
      "Epoch 439\n",
      "avg loss: 0.423527 avg acc: 0.806624\n",
      "avg val loss: 0.695570 avg val acc: 0.705556\n",
      "-------------------------------\n",
      "Epoch 440\n",
      "avg loss: 0.413165 avg acc: 0.804487\n",
      "avg val loss: 0.630905 avg val acc: 0.729167\n",
      "-------------------------------\n",
      "Epoch 441\n",
      "avg loss: 0.418851 avg acc: 0.786325\n",
      "avg val loss: 0.656130 avg val acc: 0.750000\n",
      "-------------------------------\n",
      "Epoch 442\n",
      "avg loss: 0.407941 avg acc: 0.794338\n",
      "avg val loss: 0.656128 avg val acc: 0.720833\n",
      "-------------------------------\n",
      "Epoch 443\n",
      "avg loss: 0.408822 avg acc: 0.809829\n",
      "avg val loss: 0.623084 avg val acc: 0.743056\n",
      "-------------------------------\n",
      "Epoch 444\n",
      "avg loss: 0.413150 avg acc: 0.797009\n",
      "avg val loss: 0.679784 avg val acc: 0.734722\n",
      "-------------------------------\n",
      "Epoch 445\n",
      "avg loss: 0.422325 avg acc: 0.791667\n",
      "avg val loss: 0.627337 avg val acc: 0.698611\n",
      "-------------------------------\n",
      "Epoch 446\n",
      "avg loss: 0.409840 avg acc: 0.805556\n",
      "avg val loss: 0.697374 avg val acc: 0.704167\n",
      "-------------------------------\n",
      "Epoch 447\n",
      "avg loss: 0.408889 avg acc: 0.806090\n",
      "avg val loss: 0.685947 avg val acc: 0.741667\n",
      "-------------------------------\n",
      "Epoch 448\n",
      "avg loss: 0.421062 avg acc: 0.802885\n",
      "avg val loss: 0.685851 avg val acc: 0.712500\n",
      "-------------------------------\n",
      "Epoch 449\n",
      "avg loss: 0.414595 avg acc: 0.811966\n",
      "avg val loss: 0.641018 avg val acc: 0.712500\n",
      "-------------------------------\n",
      "Epoch 450\n",
      "avg loss: 0.417663 avg acc: 0.802885\n",
      "avg val loss: 0.722104 avg val acc: 0.705556\n",
      "-------------------------------\n",
      "Epoch 451\n",
      "avg loss: 0.409589 avg acc: 0.810897\n",
      "avg val loss: 0.717537 avg val acc: 0.698611\n",
      "-------------------------------\n",
      "Epoch 452\n",
      "avg loss: 0.412948 avg acc: 0.801282\n",
      "avg val loss: 0.671441 avg val acc: 0.698611\n",
      "-------------------------------\n",
      "Epoch 453\n",
      "avg loss: 0.419626 avg acc: 0.815705\n",
      "avg val loss: 0.695570 avg val acc: 0.727778\n",
      "-------------------------------\n",
      "Epoch 454\n",
      "avg loss: 0.413847 avg acc: 0.813568\n",
      "avg val loss: 0.646312 avg val acc: 0.712500\n",
      "-------------------------------\n",
      "Epoch 455\n",
      "avg loss: 0.412819 avg acc: 0.806624\n",
      "avg val loss: 0.644450 avg val acc: 0.743056\n",
      "-------------------------------\n",
      "Epoch 456\n",
      "avg loss: 0.417851 avg acc: 0.788996\n",
      "avg val loss: 0.614672 avg val acc: 0.712500\n",
      "-------------------------------\n",
      "Epoch 457\n",
      "avg loss: 0.423551 avg acc: 0.800748\n",
      "avg val loss: 0.661435 avg val acc: 0.691667\n",
      "-------------------------------\n",
      "Epoch 458\n",
      "avg loss: 0.423927 avg acc: 0.796474\n",
      "avg val loss: 0.709465 avg val acc: 0.712500\n",
      "-------------------------------\n",
      "Epoch 459\n",
      "avg loss: 0.412941 avg acc: 0.794872\n",
      "avg val loss: 0.714040 avg val acc: 0.720833\n",
      "-------------------------------\n",
      "Epoch 460\n",
      "avg loss: 0.418698 avg acc: 0.796474\n",
      "avg val loss: 0.700039 avg val acc: 0.700000\n",
      "-------------------------------\n",
      "Epoch 461\n",
      "avg loss: 0.432528 avg acc: 0.798077\n",
      "avg val loss: 0.737032 avg val acc: 0.706944\n",
      "-------------------------------\n",
      "Epoch 462\n",
      "avg loss: 0.412402 avg acc: 0.806624\n",
      "avg val loss: 0.656229 avg val acc: 0.719444\n",
      "-------------------------------\n",
      "Epoch 463\n",
      "avg loss: 0.405526 avg acc: 0.803419\n",
      "avg val loss: 0.697965 avg val acc: 0.691667\n",
      "-------------------------------\n",
      "Epoch 464\n",
      "avg loss: 0.412774 avg acc: 0.802350\n",
      "avg val loss: 0.653173 avg val acc: 0.693056\n",
      "-------------------------------\n",
      "Epoch 465\n",
      "avg loss: 0.421240 avg acc: 0.797543\n",
      "avg val loss: 0.647939 avg val acc: 0.722222\n",
      "-------------------------------\n",
      "Epoch 466\n",
      "avg loss: 0.417919 avg acc: 0.799679\n",
      "avg val loss: 0.659274 avg val acc: 0.713889\n",
      "-------------------------------\n",
      "Epoch 467\n",
      "avg loss: 0.414246 avg acc: 0.806624\n",
      "avg val loss: 0.708942 avg val acc: 0.705556\n",
      "-------------------------------\n",
      "Epoch 468\n",
      "avg loss: 0.410591 avg acc: 0.805556\n",
      "avg val loss: 0.676058 avg val acc: 0.705556\n",
      "-------------------------------\n",
      "Epoch 469\n",
      "avg loss: 0.423220 avg acc: 0.793269\n",
      "avg val loss: 0.671780 avg val acc: 0.727778\n",
      "-------------------------------\n",
      "Epoch 470\n",
      "avg loss: 0.407016 avg acc: 0.805021\n",
      "avg val loss: 0.689489 avg val acc: 0.705556\n",
      "-------------------------------\n",
      "Epoch 471\n",
      "avg loss: 0.413717 avg acc: 0.798611\n",
      "avg val loss: 0.663941 avg val acc: 0.734722\n",
      "-------------------------------\n",
      "Epoch 472\n",
      "avg loss: 0.411441 avg acc: 0.810897\n",
      "avg val loss: 0.651888 avg val acc: 0.712500\n",
      "-------------------------------\n",
      "Epoch 473\n",
      "avg loss: 0.410570 avg acc: 0.813568\n",
      "avg val loss: 0.654440 avg val acc: 0.713889\n",
      "-------------------------------\n",
      "Epoch 474\n",
      "avg loss: 0.410526 avg acc: 0.798611\n",
      "avg val loss: 0.675913 avg val acc: 0.741667\n",
      "-------------------------------\n",
      "Epoch 475\n",
      "avg loss: 0.407925 avg acc: 0.808226\n",
      "avg val loss: 0.673442 avg val acc: 0.727778\n",
      "-------------------------------\n",
      "Epoch 476\n",
      "avg loss: 0.415633 avg acc: 0.799145\n",
      "avg val loss: 0.656211 avg val acc: 0.736111\n",
      "-------------------------------\n",
      "Epoch 477\n",
      "avg loss: 0.407763 avg acc: 0.806624\n",
      "avg val loss: 0.682811 avg val acc: 0.706944\n",
      "-------------------------------\n",
      "Epoch 478\n",
      "avg loss: 0.411422 avg acc: 0.799679\n",
      "avg val loss: 0.666303 avg val acc: 0.727778\n",
      "-------------------------------\n",
      "Epoch 479\n",
      "avg loss: 0.414222 avg acc: 0.801282\n",
      "avg val loss: 0.712749 avg val acc: 0.712500\n",
      "-------------------------------\n",
      "Epoch 480\n",
      "avg loss: 0.416342 avg acc: 0.784722\n",
      "avg val loss: 0.639969 avg val acc: 0.756944\n",
      "-------------------------------\n",
      "Epoch 481\n",
      "avg loss: 0.411189 avg acc: 0.801816\n",
      "avg val loss: 0.694403 avg val acc: 0.690278\n",
      "-------------------------------\n",
      "Epoch 482\n",
      "avg loss: 0.400624 avg acc: 0.806624\n",
      "avg val loss: 0.697203 avg val acc: 0.727778\n",
      "-------------------------------\n",
      "Epoch 483\n",
      "avg loss: 0.404707 avg acc: 0.805556\n",
      "avg val loss: 0.705604 avg val acc: 0.719444\n",
      "-------------------------------\n",
      "Epoch 484\n",
      "avg loss: 0.429446 avg acc: 0.802885\n",
      "avg val loss: 0.701197 avg val acc: 0.693056\n",
      "-------------------------------\n",
      "Epoch 485\n",
      "avg loss: 0.414776 avg acc: 0.795940\n",
      "avg val loss: 0.651078 avg val acc: 0.720833\n",
      "-------------------------------\n",
      "Epoch 486\n",
      "avg loss: 0.426061 avg acc: 0.791667\n",
      "avg val loss: 0.688803 avg val acc: 0.698611\n",
      "-------------------------------\n",
      "Epoch 487\n",
      "avg loss: 0.402886 avg acc: 0.805021\n",
      "avg val loss: 0.687278 avg val acc: 0.750000\n",
      "-------------------------------\n",
      "Epoch 488\n",
      "avg loss: 0.425566 avg acc: 0.805021\n",
      "avg val loss: 0.696870 avg val acc: 0.691667\n",
      "-------------------------------\n",
      "Epoch 489\n",
      "avg loss: 0.400930 avg acc: 0.810897\n",
      "avg val loss: 0.745667 avg val acc: 0.700000\n",
      "-------------------------------\n",
      "Epoch 490\n",
      "avg loss: 0.410918 avg acc: 0.801816\n",
      "avg val loss: 0.766492 avg val acc: 0.705556\n",
      "-------------------------------\n",
      "Epoch 491\n",
      "avg loss: 0.400069 avg acc: 0.814103\n",
      "avg val loss: 0.675049 avg val acc: 0.720833\n",
      "-------------------------------\n",
      "Epoch 492\n",
      "avg loss: 0.412317 avg acc: 0.805556\n",
      "avg val loss: 0.657375 avg val acc: 0.734722\n",
      "-------------------------------\n",
      "Epoch 493\n",
      "avg loss: 0.411280 avg acc: 0.811966\n",
      "avg val loss: 0.691646 avg val acc: 0.726389\n",
      "-------------------------------\n",
      "Epoch 494\n",
      "avg loss: 0.399775 avg acc: 0.823718\n",
      "avg val loss: 0.693323 avg val acc: 0.736111\n",
      "-------------------------------\n",
      "Epoch 495\n",
      "avg loss: 0.399649 avg acc: 0.800214\n",
      "avg val loss: 0.713463 avg val acc: 0.698611\n",
      "-------------------------------\n",
      "Epoch 496\n",
      "avg loss: 0.399439 avg acc: 0.806624\n",
      "avg val loss: 0.688248 avg val acc: 0.763889\n",
      "-------------------------------\n",
      "Epoch 497\n",
      "avg loss: 0.395728 avg acc: 0.810897\n",
      "avg val loss: 0.680679 avg val acc: 0.748611\n",
      "-------------------------------\n",
      "Epoch 498\n",
      "avg loss: 0.397411 avg acc: 0.806624\n",
      "avg val loss: 0.682620 avg val acc: 0.727778\n",
      "-------------------------------\n",
      "Epoch 499\n",
      "avg loss: 0.397634 avg acc: 0.821047\n",
      "avg val loss: 0.741314 avg val acc: 0.706944\n",
      "-------------------------------\n",
      "Epoch 500\n",
      "avg loss: 0.405039 avg acc: 0.805556\n",
      "avg val loss: 0.704785 avg val acc: 0.720833\n",
      "-------------------------------\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = config['epochs']\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer, epoch)\n",
    "    val(val_dataloader, model, loss_fn, epoch)\n",
    "    print('-------------------------------')\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td></td></tr><tr><td>loss</td><td></td></tr><tr><td>val_acc</td><td></td></tr><tr><td>val_loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>0.80556</td></tr><tr><td>loss</td><td>0.40504</td></tr><tr><td>val_acc</td><td>0.72083</td></tr><tr><td>val_loss</td><td>0.70479</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">pytorch</strong> at: <a href='https://wandb.ai/ns-super-team/assignment-1/runs/8nvz27y9' target=\"_blank\">https://wandb.ai/ns-super-team/assignment-1/runs/8nvz27y9</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230320_003713-8nvz27y9/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "variable=loss<br>index=%{x}<br>value=%{y}<extra></extra>",
         "legendgroup": "loss",
         "line": {
          "color": "#636efa",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "loss",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299,
          300,
          301,
          302,
          303,
          304,
          305,
          306,
          307,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          315,
          316,
          317,
          318,
          319,
          320,
          321,
          322,
          323,
          324,
          325,
          326,
          327,
          328,
          329,
          330,
          331,
          332,
          333,
          334,
          335,
          336,
          337,
          338,
          339,
          340,
          341,
          342,
          343,
          344,
          345,
          346,
          347,
          348,
          349,
          350,
          351,
          352,
          353,
          354,
          355,
          356,
          357,
          358,
          359,
          360,
          361,
          362,
          363,
          364,
          365,
          366,
          367,
          368,
          369,
          370,
          371,
          372,
          373,
          374,
          375,
          376,
          377,
          378,
          379,
          380,
          381,
          382,
          383,
          384,
          385,
          386,
          387,
          388,
          389,
          390,
          391,
          392,
          393,
          394,
          395,
          396,
          397,
          398,
          399,
          400,
          401,
          402,
          403,
          404,
          405,
          406,
          407,
          408,
          409,
          410,
          411,
          412,
          413,
          414,
          415,
          416,
          417,
          418,
          419,
          420,
          421,
          422,
          423,
          424,
          425,
          426,
          427,
          428,
          429,
          430,
          431,
          432,
          433,
          434,
          435,
          436,
          437,
          438,
          439,
          440,
          441,
          442,
          443,
          444,
          445,
          446,
          447,
          448,
          449,
          450,
          451,
          452,
          453,
          454,
          455,
          456,
          457,
          458,
          459,
          460,
          461,
          462,
          463,
          464,
          465,
          466,
          467,
          468,
          469,
          470,
          471,
          472,
          473,
          474,
          475,
          476,
          477,
          478,
          479,
          480,
          481,
          482,
          483,
          484,
          485,
          486,
          487,
          488,
          489,
          490,
          491,
          492,
          493,
          494,
          495,
          496,
          497,
          498,
          499
         ],
         "xaxis": "x",
         "y": [
          0.6322564192307301,
          0.6132804300540533,
          0.6197106624260927,
          0.603616456190745,
          0.603434416728142,
          0.6041705104020926,
          0.6063843942605532,
          0.6052338389249948,
          0.5987266554282262,
          0.6028429644230084,
          0.5875568107152597,
          0.5887614228786566,
          0.583853943225665,
          0.5708959492353293,
          0.5784329145382612,
          0.5653227896262438,
          0.5770172935265762,
          0.5636260845722296,
          0.5750882426897684,
          0.5650513286773975,
          0.5680300692717234,
          0.5641719355032995,
          0.5750046509962815,
          0.5668157224471753,
          0.5567170671927624,
          0.5606528428884653,
          0.5594136516253153,
          0.5606735593233353,
          0.5645178992014664,
          0.5549320800182147,
          0.5598743603779719,
          0.5572475172006167,
          0.5491168483709677,
          0.5504529185784168,
          0.5525396848336245,
          0.5553155358021076,
          0.5556244177696033,
          0.5509636440338233,
          0.5493657298577137,
          0.5513949600549845,
          0.5505684468990717,
          0.5521866083145142,
          0.5491995650988358,
          0.5467935578945355,
          0.5518291195233663,
          0.5495874881744385,
          0.5466437614881076,
          0.547742139070462,
          0.5532586452288505,
          0.5473166421437875,
          0.5426736244788537,
          0.5426817666261624,
          0.5503942462114187,
          0.5489167762108338,
          0.5433149024462088,
          0.5441413873281234,
          0.5438217245615445,
          0.5529576624051119,
          0.5462134946615268,
          0.5494101911019056,
          0.5451553051288311,
          0.5412211486926446,
          0.5411925399914767,
          0.5413506825764974,
          0.5452435100689913,
          0.5415329298911951,
          0.5413573025128781,
          0.5413780900148245,
          0.5419447697125949,
          0.5458727265015627,
          0.5471079632257804,
          0.5468473060008807,
          0.5460121165483426,
          0.5428645396843935,
          0.5382481217384338,
          0.5371386798528525,
          0.5378695917435181,
          0.5376164469963465,
          0.5416763692330091,
          0.5391688339221172,
          0.5339163465377612,
          0.5376775891352923,
          0.5355346072942783,
          0.5363313846099071,
          0.5361144642035166,
          0.5389196987335498,
          0.5329548609562409,
          0.5341251079852765,
          0.5311744121404794,
          0.5388442201492114,
          0.5347481752053286,
          0.5327533880869547,
          0.5319846341243157,
          0.5311379577869024,
          0.539743437216832,
          0.5409202109544705,
          0.5369894504547119,
          0.5382001720941983,
          0.5349311927954356,
          0.5293341936209263,
          0.5388094851603875,
          0.5374519274784968,
          0.5311125562741206,
          0.5277920105518439,
          0.5315135312386048,
          0.531697805875387,
          0.5314322305031312,
          0.5333606379154401,
          0.5330825539735647,
          0.5344873643838443,
          0.5336571626174145,
          0.5326772454457406,
          0.5284934594080999,
          0.5273417104513217,
          0.5398147572309543,
          0.5398978751439315,
          0.5404862906688299,
          0.5325023967486161,
          0.5304213051612561,
          0.5273660772886032,
          0.5283859311006008,
          0.5283751846888126,
          0.5271121171804575,
          0.5266200976494031,
          0.5214198246980325,
          0.524528193168151,
          0.5247161525946397,
          0.5268025146080897,
          0.5218392396584536,
          0.528434890967149,
          0.5205527727420514,
          0.5190452302877719,
          0.5207336506782434,
          0.5190838682345855,
          0.5214535219547076,
          0.5274499547787201,
          0.5240355531374613,
          0.5227866058166211,
          0.5228122258797671,
          0.5186194811876004,
          0.520093207175915,
          0.5241517034860758,
          0.5287077060112586,
          0.5303377864452509,
          0.5185055449987069,
          0.5161616836602871,
          0.5195933328225062,
          0.5172826945781708,
          0.5141201057495215,
          0.5176716057153848,
          0.5143732810631777,
          0.5159549556481533,
          0.5176656624445548,
          0.5154499209079987,
          0.5146955331930747,
          0.5159318997309759,
          0.5142453694954897,
          0.5168191752372644,
          0.5210733635303302,
          0.5199366219532795,
          0.5095709164937338,
          0.5126827030609815,
          0.5128106536009372,
          0.5101166260548127,
          0.5124694364957321,
          0.5138599949005322,
          0.5147932599752377,
          0.5110527957097079,
          0.5092190236617358,
          0.5109773358473411,
          0.5134166815342047,
          0.5102078314775076,
          0.5147419724708948,
          0.5077951344159933,
          0.5125535902304527,
          0.5049242782287109,
          0.5172833273044,
          0.5067322315313877,
          0.5111543406278659,
          0.5072989402673184,
          0.5073027511437734,
          0.5015657742818197,
          0.5025912584402622,
          0.511164149412742,
          0.502397643449979,
          0.5083046349195334,
          0.5043806311411735,
          0.506085991095274,
          0.5026668929136716,
          0.502605805794398,
          0.5119486030859824,
          0.5080796900467995,
          0.5057422434672331,
          0.5027928971327268,
          0.502045282186606,
          0.5022641107057914,
          0.5086680841751587,
          0.49998485354276806,
          0.5112457672754923,
          0.5022202623196137,
          0.5008894327359322,
          0.501121122103471,
          0.49865743938164836,
          0.4988549156830861,
          0.4976959274365352,
          0.5004247419345074,
          0.4974646690564278,
          0.4958647955686618,
          0.49439323559785503,
          0.49180403657448596,
          0.4937038948902717,
          0.4983483522366255,
          0.4904092198763138,
          0.49281249596522403,
          0.49242212451421297,
          0.5002097323154792,
          0.4906238592587985,
          0.49171944994192857,
          0.48749234813910264,
          0.494143593005645,
          0.48931113343972427,
          0.49070600668589276,
          0.4859510675454751,
          0.4902316805643913,
          0.4899326753921998,
          0.49542058049104154,
          0.49028614469063586,
          0.4833459189304939,
          0.4949950140256148,
          0.48937398271682936,
          0.4917565851639479,
          0.4840201513889508,
          0.4829482825902792,
          0.49097592173478544,
          0.485677638115027,
          0.49004427362711,
          0.48508743445078534,
          0.4830829845024989,
          0.4802391655170001,
          0.48324034840632707,
          0.4820126027632982,
          0.49601913873965925,
          0.4908961837108319,
          0.4832502282582797,
          0.4800877273082733,
          0.48299701588276106,
          0.477770012540695,
          0.47738631795614195,
          0.4775746036798526,
          0.48113325467476475,
          0.47570601182106215,
          0.48694144151149654,
          0.481530093229734,
          0.47512052341913563,
          0.4756534214203174,
          0.47463557047721666,
          0.4715345058685694,
          0.47509382856197846,
          0.4803368472135984,
          0.48142192990351945,
          0.4715015288346853,
          0.47860943163052583,
          0.47767681227280545,
          0.47056339031610733,
          0.47332164492362583,
          0.4750815393068852,
          0.47456726087973666,
          0.47333655831141347,
          0.47643041687133986,
          0.4739440908798805,
          0.47026161505625796,
          0.4730839545910175,
          0.4703790491972214,
          0.47090025819264925,
          0.4686622615808096,
          0.46865923893757355,
          0.4729325893597725,
          0.46882110910537916,
          0.4674168771658188,
          0.47269354875271136,
          0.46386529925542,
          0.4636790706561162,
          0.46625353395938873,
          0.4712132800083894,
          0.46013013483622134,
          0.4637413196838819,
          0.4696782334492757,
          0.4690472414860359,
          0.46312743654617894,
          0.4756326281871551,
          0.4629471386090303,
          0.46424505038139147,
          0.46290668004598373,
          0.47033295073570347,
          0.4644749370905069,
          0.4667007158964108,
          0.46480319591668934,
          0.4584237795609694,
          0.46632719116333204,
          0.4552822858095169,
          0.46893282654957896,
          0.4651989203232985,
          0.46239986327978283,
          0.46966356191879666,
          0.4692210471018767,
          0.4544557524033082,
          0.45589502767110485,
          0.45775543115077877,
          0.45671162276695937,
          0.45610665587278515,
          0.4577075602152409,
          0.4660586959276444,
          0.46772524332388854,
          0.4512967283909137,
          0.4559425570261784,
          0.45144516420670044,
          0.4647149726366385,
          0.4593886794188084,
          0.4526506371987172,
          0.4528383887731112,
          0.4473997423282036,
          0.45821672219496506,
          0.44999289894715333,
          0.4630459562326089,
          0.45125222702821094,
          0.45509119943166393,
          0.4584740339181362,
          0.4538074575173549,
          0.4520376722017924,
          0.4558851615740703,
          0.45794095060764217,
          0.4496344327926636,
          0.4518252527102446,
          0.4458551620825743,
          0.45270088391426283,
          0.4512856465119582,
          0.45697639882564545,
          0.45824393247946715,
          0.4537058480274983,
          0.4552390411114081,
          0.4481991632626607,
          0.4533424186400878,
          0.4386647366560422,
          0.44616765280564624,
          0.4488389866474347,
          0.44447160684145415,
          0.44697980888378924,
          0.4510702353257399,
          0.4425526670156381,
          0.4493478605380425,
          0.4459080336949764,
          0.45192616948714626,
          0.44786071624511326,
          0.44659441327437377,
          0.4444441577562919,
          0.43945138041789716,
          0.44819135390795195,
          0.44451182316511106,
          0.4453797351855498,
          0.44465751067186016,
          0.4428221655961795,
          0.4444755869798171,
          0.4447865402087187,
          0.44814401330092013,
          0.4405762759538797,
          0.4316543604318912,
          0.4469529688358307,
          0.43984971673060685,
          0.4398037080581372,
          0.4421221163028326,
          0.4405477864620013,
          0.44496407302526325,
          0.43519670841021413,
          0.444408580278739,
          0.431736491047419,
          0.43187676522976315,
          0.4374512907786247,
          0.4361180670750447,
          0.4366916307272055,
          0.43340466152399015,
          0.431011153337283,
          0.42618204958927935,
          0.4278145799270043,
          0.4415988475084305,
          0.44232643109101516,
          0.43701417705951595,
          0.43350738974717945,
          0.4351732054582009,
          0.42914499304233455,
          0.42605853921327835,
          0.4302743455538383,
          0.439626653224994,
          0.4282303165930968,
          0.42633615281337345,
          0.4234971014352945,
          0.432910571877773,
          0.4342613820082102,
          0.437599825935486,
          0.4326852139754173,
          0.42633522741305524,
          0.4279770935193086,
          0.4277521578165201,
          0.4311460634836784,
          0.4357068103093367,
          0.4316698828568825,
          0.42574230906290883,
          0.4365264460062369,
          0.4295743241524085,
          0.42648165501081026,
          0.426600170823244,
          0.43203829037837493,
          0.4278378089269002,
          0.4259988852800467,
          0.43165177221481615,
          0.43166994131528413,
          0.41818035336641163,
          0.42562279869348574,
          0.4389803359905879,
          0.4203306654324898,
          0.42602097262174654,
          0.42576603973523164,
          0.4255272658207478,
          0.42759733016674334,
          0.42508052900815624,
          0.4168846156352606,
          0.4276376825112563,
          0.4206684457185941,
          0.4182442663571773,
          0.41929883528978396,
          0.42117892091090864,
          0.4267932661832907,
          0.4245058328677446,
          0.4103775265125128,
          0.41655108370842076,
          0.41633950823392624,
          0.41979665672167754,
          0.41596444256794757,
          0.41331533973033613,
          0.4235265579743263,
          0.4131652995561942,
          0.4188509912062914,
          0.4079414067360071,
          0.40882181089658004,
          0.4131502302793356,
          0.42232522941552675,
          0.40984032857112396,
          0.40888910874342305,
          0.4210620480470168,
          0.41459484512989336,
          0.417662853995959,
          0.40958948853688365,
          0.412948347054995,
          0.4196258030640773,
          0.41384684160733837,
          0.4128186924335284,
          0.4178508099837181,
          0.4235512958123134,
          0.42392674279518616,
          0.41294126518261737,
          0.4186979306813998,
          0.43252762731833333,
          0.4124019470734474,
          0.4055258203775455,
          0.41277364163826674,
          0.42124013755566037,
          0.4179189254840215,
          0.41424598831396836,
          0.41059108040271663,
          0.4232195420907094,
          0.4070163518190384,
          0.413716679582229,
          0.4114409910562711,
          0.4105702527822592,
          0.41052555846862304,
          0.40792485765921765,
          0.41563297731754106,
          0.4077634173326003,
          0.41142181020516616,
          0.41422203412422764,
          0.41634220572618336,
          0.41118882252619815,
          0.40062355536680955,
          0.40470675627390545,
          0.4294458413735414,
          0.4147762075448648,
          0.4260608263504811,
          0.40288599064716923,
          0.4255664757429025,
          0.4009297765218295,
          0.410917824277511,
          0.40006902393622273,
          0.4123165038151619,
          0.4112804772762152,
          0.3997748177020978,
          0.39964853112514204,
          0.39943938186535466,
          0.39572784724908,
          0.39741056546186787,
          0.3976343102180041,
          0.4050393922206683
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "variable=val loss<br>index=%{x}<br>value=%{y}<extra></extra>",
         "legendgroup": "val loss",
         "line": {
          "color": "#EF553B",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "val loss",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299,
          300,
          301,
          302,
          303,
          304,
          305,
          306,
          307,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          315,
          316,
          317,
          318,
          319,
          320,
          321,
          322,
          323,
          324,
          325,
          326,
          327,
          328,
          329,
          330,
          331,
          332,
          333,
          334,
          335,
          336,
          337,
          338,
          339,
          340,
          341,
          342,
          343,
          344,
          345,
          346,
          347,
          348,
          349,
          350,
          351,
          352,
          353,
          354,
          355,
          356,
          357,
          358,
          359,
          360,
          361,
          362,
          363,
          364,
          365,
          366,
          367,
          368,
          369,
          370,
          371,
          372,
          373,
          374,
          375,
          376,
          377,
          378,
          379,
          380,
          381,
          382,
          383,
          384,
          385,
          386,
          387,
          388,
          389,
          390,
          391,
          392,
          393,
          394,
          395,
          396,
          397,
          398,
          399,
          400,
          401,
          402,
          403,
          404,
          405,
          406,
          407,
          408,
          409,
          410,
          411,
          412,
          413,
          414,
          415,
          416,
          417,
          418,
          419,
          420,
          421,
          422,
          423,
          424,
          425,
          426,
          427,
          428,
          429,
          430,
          431,
          432,
          433,
          434,
          435,
          436,
          437,
          438,
          439,
          440,
          441,
          442,
          443,
          444,
          445,
          446,
          447,
          448,
          449,
          450,
          451,
          452,
          453,
          454,
          455,
          456,
          457,
          458,
          459,
          460,
          461,
          462,
          463,
          464,
          465,
          466,
          467,
          468,
          469,
          470,
          471,
          472,
          473,
          474,
          475,
          476,
          477,
          478,
          479,
          480,
          481,
          482,
          483,
          484,
          485,
          486,
          487,
          488,
          489,
          490,
          491,
          492,
          493,
          494,
          495,
          496,
          497,
          498,
          499
         ],
         "xaxis": "x",
         "y": [
          0.6450513203938802,
          0.6378192106882731,
          0.6187628110249838,
          0.6226837701267667,
          0.6169457369380527,
          0.6294335491127439,
          0.6560782227251265,
          0.6364160676797231,
          0.6045385897159576,
          0.6116146412160661,
          0.6144429511494107,
          0.5890713367197249,
          0.592411564456092,
          0.5907995568381416,
          0.5765725208653344,
          0.5942660537030962,
          0.5904555122057596,
          0.6445989178286659,
          0.5928434862030877,
          0.5690367453628116,
          0.6020601391792297,
          0.6223187810844846,
          0.5738603903187646,
          0.5794556935628256,
          0.5581997334957123,
          0.5470353762308756,
          0.5655343863699172,
          0.568614168299569,
          0.5700987941688962,
          0.6195006569226583,
          0.5881598856714036,
          0.5844043360816108,
          0.5754420525497861,
          0.5720224546061622,
          0.5538149774074554,
          0.5531755685806274,
          0.5530619521935781,
          0.5606586734453837,
          0.564920402235455,
          0.6141566137472788,
          0.5427613854408264,
          0.5626551608244578,
          0.6095278362433115,
          0.6092957986725701,
          0.5392394363880157,
          0.5627673400772942,
          0.5746674802568223,
          0.545184913608763,
          0.5816929108566709,
          0.5667375690407224,
          0.5608355402946472,
          0.5562917722596062,
          0.5789945191807218,
          0.5673457980155945,
          0.5930319064193301,
          0.5626435379187266,
          0.5942250390847524,
          0.6039265029960208,
          0.5991682277785407,
          0.5831246309810214,
          0.591852456331253,
          0.5711867147021823,
          0.5697847902774811,
          0.5732724269231161,
          0.5709605515003204,
          0.5546170506212447,
          0.5862866739432017,
          0.5847082038720449,
          0.575378374920951,
          0.5771446691619025,
          0.5826862619982826,
          0.5770553482903374,
          0.6156564089987013,
          0.5455830759472318,
          0.6040064493815104,
          0.5575295918517642,
          0.6034983860121833,
          0.5485944847265879,
          0.5603702002101474,
          0.5861804783344269,
          0.5460328989558749,
          0.543798867199156,
          0.5633361637592316,
          0.5414146184921265,
          0.5575122899479337,
          0.5800781216886308,
          0.5702159470982022,
          0.5356237457858192,
          0.5633887085649703,
          0.5408226152261099,
          0.5572019086943732,
          0.5736696951919131,
          0.57701419128312,
          0.5776934623718262,
          0.5690281589825948,
          0.5961459709538354,
          0.5920640130837759,
          0.5556101500988007,
          0.5570186211003197,
          0.5958778460820516,
          0.5788746906651391,
          0.5436754723389944,
          0.5735626982318031,
          0.5779166155391269,
          0.5513215495480431,
          0.6262235906389024,
          0.551013148493237,
          0.5308142635557387,
          0.6048854986826578,
          0.5794563061661191,
          0.5502672162320879,
          0.5501217643419901,
          0.5403320789337158,
          0.5493154095278846,
          0.5996993846363492,
          0.5460901922649808,
          0.5963857902420892,
          0.5411053564813402,
          0.559876001543469,
          0.5587707459926605,
          0.5667830771870084,
          0.5551210873656802,
          0.5613887740506066,
          0.5695152348942227,
          0.564011898305681,
          0.561181922753652,
          0.6344747013515897,
          0.5461680028173659,
          0.5492058760590024,
          0.5491158465544382,
          0.5651621520519257,
          0.5529458622137705,
          0.5406796965334151,
          0.5448681248558892,
          0.5498018132315742,
          0.5900216400623322,
          0.5402082006136576,
          0.5960194733407762,
          0.5856090386708578,
          0.5407123201423221,
          0.6095837950706482,
          0.565844843784968,
          0.5433491286304262,
          0.5738071137004428,
          0.5709724558724297,
          0.5503453579213884,
          0.5660292539331648,
          0.5602179798814986,
          0.5569896797339121,
          0.531858368880219,
          0.554948220650355,
          0.5495706266827054,
          0.5722323358058929,
          0.5885930822955238,
          0.5457776917351617,
          0.5429105162620544,
          0.5478675994608138,
          0.6072502235571543,
          0.5676005085309347,
          0.5662809875276353,
          0.5579960644245148,
          0.5544010632567935,
          0.5534598098860847,
          0.5747369527816772,
          0.5633468495474921,
          0.5846531887849172,
          0.5683884355756972,
          0.5543040235837301,
          0.574748565753301,
          0.5622942513889737,
          0.6457211209668053,
          0.5987468792332543,
          0.5682030949327681,
          0.5573888487286038,
          0.5752043724060059,
          0.574723482131958,
          0.6306529972288344,
          0.5666490859455533,
          0.5629909303453233,
          0.6244533922937181,
          0.5516115327676138,
          0.6365946465068393,
          0.5516208708286285,
          0.5717860592736138,
          0.6043727331691318,
          0.6063426931699117,
          0.5771436625056796,
          0.5456987884309557,
          0.5696844293011559,
          0.6244470377763113,
          0.6501890387800005,
          0.5870258576340146,
          0.5585825244585673,
          0.6068481769826677,
          0.5591453280713823,
          0.6237069964408875,
          0.5924578110376993,
          0.5918126470512814,
          0.5842625697453817,
          0.556747148434321,
          0.6077415115303464,
          0.6231791343953874,
          0.5580457515186734,
          0.595399303568734,
          0.5774102310339609,
          0.5542710853947533,
          0.5597279402944777,
          0.647773050599628,
          0.5953919821315341,
          0.6029875841405656,
          0.5584899187088013,
          0.6019580231772529,
          0.6181322832902273,
          0.5858618153466119,
          0.5697327852249146,
          0.543604807721244,
          0.6062554683950212,
          0.5768092307779524,
          0.5851617289914025,
          0.634933825996187,
          0.5734501116805606,
          0.6310332583056556,
          0.5722620851463742,
          0.5650075144237943,
          0.5848535034391615,
          0.6083684431182014,
          0.5632469289832644,
          0.6079093780782487,
          0.5533187687397003,
          0.6060430241955651,
          0.5604158739248911,
          0.5669951770040724,
          0.6135204798645444,
          0.57579172650973,
          0.5894626478354136,
          0.5910911824968126,
          0.586414227883021,
          0.5846098297172122,
          0.5964961813555824,
          0.592986438009474,
          0.558323515786065,
          0.6306839287281036,
          0.5901493264569176,
          0.6272332469622294,
          0.602613263660007,
          0.5763762295246124,
          0.603279365433587,
          0.6143823365370432,
          0.5884961552090116,
          0.584189020925098,
          0.621656682756212,
          0.5671567022800446,
          0.5631435447269015,
          0.5791439943843417,
          0.5953333775202433,
          0.5704978803793589,
          0.5788790351814694,
          0.5891640981038412,
          0.6262899570994906,
          0.5790975689888,
          0.6490189068847232,
          0.5763916273911794,
          0.5992674794461992,
          0.6082506246036954,
          0.6092983053790199,
          0.6132382286919488,
          0.6531830231348673,
          0.6576615787214704,
          0.5783224602540334,
          0.6182017425696055,
          0.5800677273008559,
          0.6148771312501695,
          0.665122903055615,
          0.5658068855603536,
          0.5997002489036984,
          0.5921209851900736,
          0.6778719425201416,
          0.6137310829427507,
          0.5812697973516252,
          0.6265635357962714,
          0.5842196709579892,
          0.6322759389877319,
          0.6340981423854828,
          0.5987463361687131,
          0.6337278683980306,
          0.6076123317082723,
          0.7097089423073663,
          0.6074184411101871,
          0.6246287789609697,
          0.6195192403263516,
          0.6286701030201383,
          0.5783902572260963,
          0.6066458490159776,
          0.5914820267094506,
          0.5993258323934343,
          0.5487513012356229,
          0.6043290959464179,
          0.6502708262867398,
          0.5973740451865726,
          0.6452741175889969,
          0.5685677594608731,
          0.66187474793858,
          0.622224297788408,
          0.6214019507169724,
          0.6605703168445163,
          0.6159451935026381,
          0.6138197580973307,
          0.5939949817127652,
          0.6597878734270731,
          0.6200588080618117,
          0.6293030811680688,
          0.6189554035663605,
          0.677539779080285,
          0.5565847986274295,
          0.5804212126466963,
          0.6517080763975779,
          0.6595967047744327,
          0.5976933207776811,
          0.6532899373107486,
          0.6452679832776388,
          0.6138505505190955,
          0.6800618535942502,
          0.6240827010737525,
          0.6242429978317685,
          0.6521935860315958,
          0.6372982296678755,
          0.7007143563694425,
          0.5894101245535744,
          0.6049389640490214,
          0.6221419076124827,
          0.6167458693186442,
          0.6190376083056132,
          0.6601332161161635,
          0.5886061787605286,
          0.6976092391543918,
          0.6110504733191596,
          0.6669363677501678,
          0.6486911806795332,
          0.6056139104896121,
          0.583727240562439,
          0.6457855304082235,
          0.6182093156708611,
          0.6070178382926517,
          0.6112906535466512,
          0.6629178788926866,
          0.6214741534656949,
          0.6970436142550575,
          0.6161172389984131,
          0.6352677775753869,
          0.6352608667479621,
          0.6522079077031877,
          0.6320374839835696,
          0.6792841321892209,
          0.6802789833810594,
          0.6423714359601339,
          0.7695675260490842,
          0.674435900317298,
          0.6350240177578397,
          0.6746437615818448,
          0.6499709884325663,
          0.5896516839663187,
          0.6375710037019517,
          0.62969784769747,
          0.6415870901611116,
          0.5937132272455428,
          0.6278115212917328,
          0.6148305667771233,
          0.6581318212880028,
          0.6259333690007528,
          0.6134884423679776,
          0.5828976598050859,
          0.6423928406503465,
          0.6601614703734716,
          0.661091321044498,
          0.6392222377989027,
          0.649014006058375,
          0.5958727035257552,
          0.6425123777654436,
          0.59457728266716,
          0.6793594658374786,
          0.6402676834000481,
          0.6045814686351352,
          0.6435146099991269,
          0.6328561239772372,
          0.6390787594848208,
          0.6244712902439965,
          0.7190725306669871,
          0.622094968954722,
          0.6171511312325796,
          0.6970000565052032,
          0.6592933105097877,
          0.6823963092433082,
          0.6208333041932848,
          0.66485153304206,
          0.708990087111791,
          0.6276048719882965,
          0.6495357983642154,
          0.6292942298783196,
          0.7419172492292192,
          0.6956987645890977,
          0.6803305745124817,
          0.6506743497318692,
          0.6279692285590701,
          0.6389717525906033,
          0.6320102512836456,
          0.6222739484575059,
          0.6528610156642066,
          0.6666890250311958,
          0.6034214066134559,
          0.6601935625076294,
          0.7073076334264543,
          0.594592735171318,
          0.6345726350943247,
          0.6597384280628629,
          0.6423216296566857,
          0.6635739273495145,
          0.6023123297426436,
          0.6765897737609016,
          0.618659132056766,
          0.6718028750684526,
          0.6254289050896963,
          0.7435816393958198,
          0.6745674676365323,
          0.6208936903211806,
          0.6575366722212898,
          0.6193805535634359,
          0.6392216980457306,
          0.611318772037824,
          0.6771765086385939,
          0.6348903973897299,
          0.6807177795304192,
          0.6155866119596693,
          0.62885872191853,
          0.7680004984140396,
          0.68466626935535,
          0.672108554177814,
          0.6559702687793307,
          0.655672636297014,
          0.6955696410602994,
          0.630904903014501,
          0.6561303635438284,
          0.656128035651313,
          0.6230841130018234,
          0.6797836224238077,
          0.6273367934756808,
          0.6973740557829539,
          0.6859471963511573,
          0.6858511832025316,
          0.6410179932912191,
          0.7221043937736087,
          0.7175365024142795,
          0.6714408563243018,
          0.6955704357888963,
          0.6463124056657156,
          0.6444501810603671,
          0.6146716508600447,
          0.6614349616898431,
          0.7094653844833374,
          0.7140395707554288,
          0.700038880109787,
          0.7370316253768073,
          0.6562287476327684,
          0.6979653040568033,
          0.6531728969679939,
          0.6479385644197464,
          0.6592743860350715,
          0.708942198091083,
          0.6760582294729021,
          0.6717804570992788,
          0.6894891129599677,
          0.6639405853218503,
          0.651888499657313,
          0.6544402804639604,
          0.6759131252765656,
          0.6734423471821679,
          0.6562113596333398,
          0.6828107900089688,
          0.6663031379381815,
          0.7127493917942047,
          0.6399692611561881,
          0.6944029629230499,
          0.6972030036979251,
          0.7056038843260871,
          0.7011968890825907,
          0.6510777274767557,
          0.6888034012582567,
          0.687277658118142,
          0.6968697243266635,
          0.7456673814190758,
          0.7664918700853983,
          0.6750493612554338,
          0.6573746916320589,
          0.6916462414794498,
          0.6933233274353875,
          0.7134634984864129,
          0.6882477088107003,
          0.6806794140073988,
          0.6826202107800378,
          0.7413142985767789,
          0.704785062207116
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "title": {
          "text": "variable"
         },
         "tracegroupgap": 0
        },
        "margin": {
         "t": 60
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "index"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "value"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = px.line({'loss': loss_graph, 'val loss': val_loss_graph})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "variable=acc<br>index=%{x}<br>value=%{y}<extra></extra>",
         "legendgroup": "acc",
         "line": {
          "color": "#636efa",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "acc",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299,
          300,
          301,
          302,
          303,
          304,
          305,
          306,
          307,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          315,
          316,
          317,
          318,
          319,
          320,
          321,
          322,
          323,
          324,
          325,
          326,
          327,
          328,
          329,
          330,
          331,
          332,
          333,
          334,
          335,
          336,
          337,
          338,
          339,
          340,
          341,
          342,
          343,
          344,
          345,
          346,
          347,
          348,
          349,
          350,
          351,
          352,
          353,
          354,
          355,
          356,
          357,
          358,
          359,
          360,
          361,
          362,
          363,
          364,
          365,
          366,
          367,
          368,
          369,
          370,
          371,
          372,
          373,
          374,
          375,
          376,
          377,
          378,
          379,
          380,
          381,
          382,
          383,
          384,
          385,
          386,
          387,
          388,
          389,
          390,
          391,
          392,
          393,
          394,
          395,
          396,
          397,
          398,
          399,
          400,
          401,
          402,
          403,
          404,
          405,
          406,
          407,
          408,
          409,
          410,
          411,
          412,
          413,
          414,
          415,
          416,
          417,
          418,
          419,
          420,
          421,
          422,
          423,
          424,
          425,
          426,
          427,
          428,
          429,
          430,
          431,
          432,
          433,
          434,
          435,
          436,
          437,
          438,
          439,
          440,
          441,
          442,
          443,
          444,
          445,
          446,
          447,
          448,
          449,
          450,
          451,
          452,
          453,
          454,
          455,
          456,
          457,
          458,
          459,
          460,
          461,
          462,
          463,
          464,
          465,
          466,
          467,
          468,
          469,
          470,
          471,
          472,
          473,
          474,
          475,
          476,
          477,
          478,
          479,
          480,
          481,
          482,
          483,
          484,
          485,
          486,
          487,
          488,
          489,
          490,
          491,
          492,
          493,
          494,
          495,
          496,
          497,
          498,
          499
         ],
         "xaxis": "x",
         "y": [
          0.6666666666666666,
          0.6816239316239316,
          0.6981837606837608,
          0.7051282051282052,
          0.6949786324786326,
          0.6971153846153846,
          0.7147435897435898,
          0.6768162393162392,
          0.7254273504273504,
          0.7115384615384616,
          0.7083333333333334,
          0.7232905982905983,
          0.7248931623931624,
          0.719551282051282,
          0.7382478632478633,
          0.7264957264957265,
          0.7131410256410257,
          0.7371794871794872,
          0.7174145299145299,
          0.7323717948717948,
          0.7238247863247863,
          0.7259615384615384,
          0.7147435897435898,
          0.7222222222222222,
          0.7275641025641025,
          0.7248931623931624,
          0.7206196581196581,
          0.7291666666666666,
          0.7248931623931624,
          0.7158119658119658,
          0.7275641025641025,
          0.7291666666666666,
          0.7302350427350428,
          0.7264957264957265,
          0.7291666666666666,
          0.718482905982906,
          0.7339743589743589,
          0.7318376068376069,
          0.7291666666666666,
          0.7286324786324787,
          0.7280982905982906,
          0.7307692307692307,
          0.7313034188034188,
          0.7227564102564102,
          0.7297008547008547,
          0.7259615384615384,
          0.7254273504273504,
          0.7206196581196581,
          0.7248931623931624,
          0.7254273504273504,
          0.7248931623931624,
          0.7275641025641025,
          0.7264957264957265,
          0.7259615384615384,
          0.7254273504273504,
          0.7280982905982906,
          0.7280982905982906,
          0.7382478632478633,
          0.7318376068376069,
          0.7227564102564102,
          0.7275641025641025,
          0.7270299145299146,
          0.7254273504273504,
          0.734508547008547,
          0.7302350427350428,
          0.7371794871794872,
          0.7339743589743589,
          0.7297008547008547,
          0.734508547008547,
          0.7387820512820513,
          0.7323717948717948,
          0.7371794871794872,
          0.7307692307692307,
          0.734508547008547,
          0.7254273504273504,
          0.7280982905982906,
          0.7302350427350428,
          0.7377136752136751,
          0.7264957264957265,
          0.733440170940171,
          0.7329059829059829,
          0.7323717948717948,
          0.7329059829059829,
          0.7307692307692307,
          0.7297008547008547,
          0.736111111111111,
          0.7377136752136751,
          0.7350427350427351,
          0.7350427350427351,
          0.7248931623931624,
          0.7307692307692307,
          0.7366452991452992,
          0.7350427350427351,
          0.7355769230769231,
          0.7302350427350428,
          0.734508547008547,
          0.733440170940171,
          0.736111111111111,
          0.7323717948717948,
          0.7377136752136751,
          0.7409188034188033,
          0.7318376068376069,
          0.7430555555555556,
          0.7387820512820513,
          0.736111111111111,
          0.734508547008547,
          0.7387820512820513,
          0.734508547008547,
          0.7419871794871795,
          0.7329059829059829,
          0.7403846153846154,
          0.7382478632478633,
          0.736111111111111,
          0.7339743589743589,
          0.7382478632478633,
          0.734508547008547,
          0.7398504273504274,
          0.7403846153846154,
          0.7403846153846154,
          0.7430555555555556,
          0.7409188034188033,
          0.7419871794871795,
          0.7462606837606838,
          0.7489316239316239,
          0.7419871794871795,
          0.7403846153846154,
          0.7430555555555556,
          0.7366452991452992,
          0.7430555555555556,
          0.7398504273504274,
          0.7366452991452992,
          0.7451923076923077,
          0.7409188034188033,
          0.7473290598290598,
          0.749465811965812,
          0.7329059829059829,
          0.7462606837606838,
          0.7403846153846154,
          0.7398504273504274,
          0.7371794871794872,
          0.7441239316239316,
          0.7451923076923077,
          0.7419871794871795,
          0.7339743589743589,
          0.7473290598290598,
          0.7409188034188033,
          0.7483974358974359,
          0.7414529914529915,
          0.7467948717948718,
          0.7419871794871795,
          0.7473290598290598,
          0.750534188034188,
          0.7435897435897436,
          0.7478632478632479,
          0.7489316239316239,
          0.7473290598290598,
          0.7409188034188033,
          0.7473290598290598,
          0.7398504273504274,
          0.7441239316239316,
          0.7478632478632479,
          0.7473290598290598,
          0.750534188034188,
          0.7451923076923077,
          0.7430555555555556,
          0.7393162393162392,
          0.7393162393162392,
          0.7441239316239316,
          0.7553418803418803,
          0.7548076923076923,
          0.7441239316239316,
          0.75,
          0.7553418803418803,
          0.7446581196581197,
          0.7467948717948718,
          0.7532051282051282,
          0.7382478632478633,
          0.749465811965812,
          0.7425213675213674,
          0.75,
          0.7473290598290598,
          0.7521367521367521,
          0.7451923076923077,
          0.7409188034188033,
          0.7467948717948718,
          0.7467948717948718,
          0.7548076923076923,
          0.7483974358974359,
          0.7553418803418803,
          0.7564102564102564,
          0.75,
          0.7473290598290598,
          0.7473290598290598,
          0.75,
          0.750534188034188,
          0.749465811965812,
          0.7510683760683761,
          0.7521367521367521,
          0.7532051282051282,
          0.7451923076923077,
          0.7542735042735043,
          0.7569444444444444,
          0.7377136752136751,
          0.7606837606837608,
          0.75,
          0.75,
          0.7489316239316239,
          0.7537393162393162,
          0.7489316239316239,
          0.7574786324786326,
          0.7489316239316239,
          0.7457264957264957,
          0.7473290598290598,
          0.7542735042735043,
          0.7580128205128205,
          0.7489316239316239,
          0.7628205128205128,
          0.7622863247863249,
          0.7548076923076923,
          0.7564102564102564,
          0.749465811965812,
          0.7580128205128205,
          0.7532051282051282,
          0.7644230769230769,
          0.7569444444444444,
          0.750534188034188,
          0.7451923076923077,
          0.7521367521367521,
          0.7457264957264957,
          0.7644230769230769,
          0.7719017094017094,
          0.749465811965812,
          0.7681623931623931,
          0.7564102564102564,
          0.7564102564102564,
          0.7649572649572649,
          0.7516025641025641,
          0.7558760683760684,
          0.7697649572649572,
          0.7553418803418803,
          0.7649572649572649,
          0.7430555555555556,
          0.7617521367521367,
          0.7617521367521367,
          0.7612179487179487,
          0.7670940170940171,
          0.7612179487179487,
          0.7644230769230769,
          0.7724358974358975,
          0.7553418803418803,
          0.7724358974358975,
          0.7574786324786326,
          0.763888888888889,
          0.7793803418803419,
          0.7681623931623931,
          0.763888888888889,
          0.7686965811965812,
          0.7777777777777778,
          0.763888888888889,
          0.766559829059829,
          0.7745726495726496,
          0.7628205128205128,
          0.7606837606837608,
          0.7740384615384616,
          0.7590811965811967,
          0.7719017094017094,
          0.75,
          0.7735042735042735,
          0.7590811965811967,
          0.7681623931623931,
          0.7793803418803419,
          0.7692307692307693,
          0.7719017094017094,
          0.7799145299145299,
          0.766559829059829,
          0.7702991452991453,
          0.7628205128205128,
          0.7729700854700854,
          0.781517094017094,
          0.7788461538461539,
          0.7783119658119658,
          0.7852564102564102,
          0.7761752136752137,
          0.7708333333333334,
          0.7772435897435898,
          0.7713675213675213,
          0.7670940170940171,
          0.7745726495726496,
          0.7799145299145299,
          0.780448717948718,
          0.7729700854700854,
          0.765491452991453,
          0.7793803418803419,
          0.7622863247863249,
          0.7697649572649572,
          0.7841880341880342,
          0.7788461538461539,
          0.780448717948718,
          0.7756410256410257,
          0.782051282051282,
          0.7783119658119658,
          0.7740384615384616,
          0.7756410256410257,
          0.7729700854700854,
          0.7574786324786326,
          0.7879273504273504,
          0.7847222222222222,
          0.7863247863247863,
          0.7735042735042735,
          0.781517094017094,
          0.7788461538461539,
          0.7713675213675213,
          0.7783119658119658,
          0.7900641025641025,
          0.7847222222222222,
          0.7863247863247863,
          0.7622863247863249,
          0.781517094017094,
          0.7884615384615384,
          0.7900641025641025,
          0.7868589743589743,
          0.7745726495726496,
          0.7831196581196581,
          0.7612179487179487,
          0.7932692307692307,
          0.782051282051282,
          0.7729700854700854,
          0.782051282051282,
          0.7911324786324787,
          0.7825854700854701,
          0.7852564102564102,
          0.7873931623931624,
          0.7831196581196581,
          0.7922008547008547,
          0.7836538461538461,
          0.782051282051282,
          0.7799145299145299,
          0.7708333333333334,
          0.7767094017094017,
          0.7751068376068376,
          0.7825854700854701,
          0.7879273504273504,
          0.7879273504273504,
          0.7916666666666666,
          0.7777777777777778,
          0.7767094017094017,
          0.7900641025641025,
          0.7879273504273504,
          0.7831196581196581,
          0.7735042735042735,
          0.7954059829059829,
          0.7745726495726496,
          0.7863247863247863,
          0.7756410256410257,
          0.7943376068376069,
          0.7873931623931624,
          0.781517094017094,
          0.7964743589743589,
          0.7927350427350428,
          0.7900641025641025,
          0.7948717948717948,
          0.7932692307692307,
          0.7879273504273504,
          0.7911324786324787,
          0.7873931623931624,
          0.8002136752136751,
          0.7799145299145299,
          0.7884615384615384,
          0.7767094017094017,
          0.7980769230769231,
          0.8007478632478633,
          0.7841880341880342,
          0.7975427350427351,
          0.7852564102564102,
          0.798611111111111,
          0.795940170940171,
          0.7884615384615384,
          0.7905982905982906,
          0.7852564102564102,
          0.7884615384615384,
          0.7911324786324787,
          0.795940170940171,
          0.8039529914529915,
          0.7911324786324787,
          0.7836538461538461,
          0.7831196581196581,
          0.7954059829059829,
          0.7922008547008547,
          0.7911324786324787,
          0.7980769230769231,
          0.7895299145299146,
          0.7831196581196581,
          0.797008547008547,
          0.7879273504273504,
          0.795940170940171,
          0.7916666666666666,
          0.8023504273504274,
          0.7841880341880342,
          0.795940170940171,
          0.7927350427350428,
          0.797008547008547,
          0.7948717948717948,
          0.8002136752136751,
          0.7873931623931624,
          0.7938034188034188,
          0.8023504273504274,
          0.7772435897435898,
          0.8034188034188033,
          0.7980769230769231,
          0.7868589743589743,
          0.7873931623931624,
          0.7932692307692307,
          0.7911324786324787,
          0.7916666666666666,
          0.8002136752136751,
          0.8044871794871795,
          0.798611111111111,
          0.7916666666666666,
          0.8055555555555556,
          0.795940170940171,
          0.7932692307692307,
          0.8002136752136751,
          0.7911324786324787,
          0.795940170940171,
          0.797008547008547,
          0.7889957264957265,
          0.797008547008547,
          0.795940170940171,
          0.797008547008547,
          0.8002136752136751,
          0.798611111111111,
          0.7964743589743589,
          0.8087606837606838,
          0.7911324786324787,
          0.7927350427350428,
          0.8012820512820513,
          0.8092948717948718,
          0.8007478632478633,
          0.8066239316239316,
          0.8044871794871795,
          0.7863247863247863,
          0.7943376068376069,
          0.8098290598290598,
          0.797008547008547,
          0.7916666666666666,
          0.8055555555555556,
          0.8060897435897436,
          0.8028846153846154,
          0.811965811965812,
          0.8028846153846154,
          0.8108974358974359,
          0.8012820512820513,
          0.8157051282051282,
          0.8135683760683761,
          0.8066239316239316,
          0.7889957264957265,
          0.8007478632478633,
          0.7964743589743589,
          0.7948717948717948,
          0.7964743589743589,
          0.7980769230769231,
          0.8066239316239316,
          0.8034188034188033,
          0.8023504273504274,
          0.7975427350427351,
          0.7996794871794872,
          0.8066239316239316,
          0.8055555555555556,
          0.7932692307692307,
          0.8050213675213674,
          0.798611111111111,
          0.8108974358974359,
          0.8135683760683761,
          0.798611111111111,
          0.8082264957264957,
          0.7991452991452992,
          0.8066239316239316,
          0.7996794871794872,
          0.8012820512820513,
          0.7847222222222222,
          0.8018162393162392,
          0.8066239316239316,
          0.8055555555555556,
          0.8028846153846154,
          0.795940170940171,
          0.7916666666666666,
          0.8050213675213674,
          0.8050213675213674,
          0.8108974358974359,
          0.8018162393162392,
          0.8141025641025641,
          0.8055555555555556,
          0.811965811965812,
          0.8237179487179487,
          0.8002136752136751,
          0.8066239316239316,
          0.8108974358974359,
          0.8066239316239316,
          0.8210470085470086,
          0.8055555555555556
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "variable=val acc<br>index=%{x}<br>value=%{y}<extra></extra>",
         "legendgroup": "val acc",
         "line": {
          "color": "#EF553B",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "val acc",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299,
          300,
          301,
          302,
          303,
          304,
          305,
          306,
          307,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          315,
          316,
          317,
          318,
          319,
          320,
          321,
          322,
          323,
          324,
          325,
          326,
          327,
          328,
          329,
          330,
          331,
          332,
          333,
          334,
          335,
          336,
          337,
          338,
          339,
          340,
          341,
          342,
          343,
          344,
          345,
          346,
          347,
          348,
          349,
          350,
          351,
          352,
          353,
          354,
          355,
          356,
          357,
          358,
          359,
          360,
          361,
          362,
          363,
          364,
          365,
          366,
          367,
          368,
          369,
          370,
          371,
          372,
          373,
          374,
          375,
          376,
          377,
          378,
          379,
          380,
          381,
          382,
          383,
          384,
          385,
          386,
          387,
          388,
          389,
          390,
          391,
          392,
          393,
          394,
          395,
          396,
          397,
          398,
          399,
          400,
          401,
          402,
          403,
          404,
          405,
          406,
          407,
          408,
          409,
          410,
          411,
          412,
          413,
          414,
          415,
          416,
          417,
          418,
          419,
          420,
          421,
          422,
          423,
          424,
          425,
          426,
          427,
          428,
          429,
          430,
          431,
          432,
          433,
          434,
          435,
          436,
          437,
          438,
          439,
          440,
          441,
          442,
          443,
          444,
          445,
          446,
          447,
          448,
          449,
          450,
          451,
          452,
          453,
          454,
          455,
          456,
          457,
          458,
          459,
          460,
          461,
          462,
          463,
          464,
          465,
          466,
          467,
          468,
          469,
          470,
          471,
          472,
          473,
          474,
          475,
          476,
          477,
          478,
          479,
          480,
          481,
          482,
          483,
          484,
          485,
          486,
          487,
          488,
          489,
          490,
          491,
          492,
          493,
          494,
          495,
          496,
          497,
          498,
          499
         ],
         "xaxis": "x",
         "y": [
          0.6208333333333333,
          0.6861111111111111,
          0.6291666666666667,
          0.6499999999999999,
          0.6513888888888889,
          0.6722222222222222,
          0.6083333333333333,
          0.6902777777777778,
          0.7416666666666667,
          0.7,
          0.6972222222222223,
          0.7361111111111112,
          0.7,
          0.6972222222222223,
          0.7361111111111112,
          0.7208333333333333,
          0.7194444444444444,
          0.6305555555555555,
          0.7208333333333333,
          0.7208333333333333,
          0.6972222222222223,
          0.7,
          0.7055555555555555,
          0.7208333333333333,
          0.7208333333333333,
          0.7291666666666666,
          0.7055555555555555,
          0.6902777777777778,
          0.7138888888888889,
          0.6777777777777777,
          0.7055555555555555,
          0.7055555555555555,
          0.7124999999999999,
          0.6986111111111111,
          0.7138888888888889,
          0.7,
          0.7208333333333333,
          0.7208333333333333,
          0.7277777777777777,
          0.6847222222222222,
          0.7208333333333333,
          0.6986111111111111,
          0.6680555555555556,
          0.7055555555555555,
          0.7430555555555556,
          0.6986111111111111,
          0.7124999999999999,
          0.7361111111111112,
          0.7208333333333333,
          0.7138888888888889,
          0.7055555555555555,
          0.7124999999999999,
          0.6916666666666667,
          0.7055555555555555,
          0.6986111111111111,
          0.6986111111111111,
          0.6763888888888889,
          0.7138888888888889,
          0.6763888888888889,
          0.6986111111111111,
          0.6986111111111111,
          0.6694444444444445,
          0.6986111111111111,
          0.6916666666666667,
          0.7138888888888889,
          0.7277777777777777,
          0.7055555555555555,
          0.7055555555555555,
          0.6986111111111111,
          0.7194444444444444,
          0.7208333333333333,
          0.6986111111111111,
          0.6902777777777778,
          0.7430555555555556,
          0.7055555555555555,
          0.7347222222222222,
          0.6763888888888889,
          0.7277777777777777,
          0.6902777777777778,
          0.7208333333333333,
          0.7138888888888889,
          0.7222222222222222,
          0.7055555555555555,
          0.7277777777777777,
          0.7361111111111112,
          0.6680555555555556,
          0.7208333333333333,
          0.7361111111111112,
          0.7277777777777777,
          0.7291666666666666,
          0.7055555555555555,
          0.6986111111111111,
          0.6833333333333333,
          0.6972222222222223,
          0.7055555555555555,
          0.6902777777777778,
          0.6902777777777778,
          0.7208333333333333,
          0.7277777777777777,
          0.7055555555555555,
          0.7055555555555555,
          0.7208333333333333,
          0.7055555555555555,
          0.6819444444444445,
          0.7208333333333333,
          0.6833333333333333,
          0.7277777777777777,
          0.75,
          0.6902777777777778,
          0.7055555555555555,
          0.7208333333333333,
          0.7291666666666666,
          0.7361111111111112,
          0.7291666666666666,
          0.6833333333333333,
          0.7138888888888889,
          0.7124999999999999,
          0.7277777777777777,
          0.7138888888888889,
          0.7138888888888889,
          0.6986111111111111,
          0.7291666666666666,
          0.6986111111111111,
          0.7124999999999999,
          0.7138888888888889,
          0.7124999999999999,
          0.6902777777777778,
          0.7291666666666666,
          0.7055555555555555,
          0.7208333333333333,
          0.7124999999999999,
          0.7291666666666666,
          0.7208333333333333,
          0.7208333333333333,
          0.7291666666666666,
          0.6986111111111111,
          0.7208333333333333,
          0.6833333333333333,
          0.6972222222222223,
          0.7430555555555556,
          0.6902777777777778,
          0.7138888888888889,
          0.7361111111111112,
          0.7194444444444444,
          0.7124999999999999,
          0.7138888888888889,
          0.7138888888888889,
          0.7208333333333333,
          0.7055555555555555,
          0.7222222222222222,
          0.7277777777777777,
          0.7222222222222222,
          0.7277777777777777,
          0.6986111111111111,
          0.7138888888888889,
          0.7069444444444444,
          0.7208333333333333,
          0.6833333333333333,
          0.7194444444444444,
          0.7208333333333333,
          0.7277777777777777,
          0.7277777777777777,
          0.7208333333333333,
          0.7055555555555555,
          0.7361111111111112,
          0.6833333333333333,
          0.7055555555555555,
          0.7277777777777777,
          0.7194444444444444,
          0.7208333333333333,
          0.6986111111111111,
          0.7055555555555555,
          0.7347222222222222,
          0.7361111111111112,
          0.7277777777777777,
          0.7124999999999999,
          0.6902777777777778,
          0.7055555555555555,
          0.7361111111111112,
          0.6972222222222223,
          0.7361111111111112,
          0.6819444444444445,
          0.7361111111111112,
          0.7277777777777777,
          0.6972222222222223,
          0.6972222222222223,
          0.7277777777777777,
          0.75,
          0.7277777777777777,
          0.6972222222222223,
          0.6902777777777778,
          0.7124999999999999,
          0.7430555555555556,
          0.7055555555555555,
          0.7208333333333333,
          0.6972222222222223,
          0.7430555555555556,
          0.7263888888888889,
          0.7194444444444444,
          0.75,
          0.7055555555555555,
          0.7124999999999999,
          0.7277777777777777,
          0.7194444444444444,
          0.7194444444444444,
          0.7416666666666667,
          0.7430555555555556,
          0.6902777777777778,
          0.7055555555555555,
          0.7347222222222222,
          0.7555555555555555,
          0.7055555555555555,
          0.7124999999999999,
          0.7194444444444444,
          0.7347222222222222,
          0.7361111111111112,
          0.7277777777777777,
          0.7430555555555556,
          0.6986111111111111,
          0.6902777777777778,
          0.7208333333333333,
          0.7194444444444444,
          0.7486111111111111,
          0.7291666666666666,
          0.6986111111111111,
          0.7347222222222222,
          0.75,
          0.7055555555555555,
          0.7708333333333334,
          0.7041666666666667,
          0.7361111111111112,
          0.7416666666666667,
          0.7277777777777777,
          0.75,
          0.7277777777777777,
          0.7347222222222222,
          0.7277777777777777,
          0.7277777777777777,
          0.6902777777777778,
          0.7194444444444444,
          0.7138888888888889,
          0.6777777777777777,
          0.7208333333333333,
          0.6902777777777778,
          0.7347222222222222,
          0.7277777777777777,
          0.7055555555555555,
          0.7055555555555555,
          0.6972222222222223,
          0.7194444444444444,
          0.7055555555555555,
          0.7430555555555556,
          0.75,
          0.6847222222222222,
          0.7347222222222222,
          0.7208333333333333,
          0.7277777777777777,
          0.7277777777777777,
          0.7347222222222222,
          0.7055555555555555,
          0.6694444444444445,
          0.7208333333333333,
          0.7055555555555555,
          0.7055555555555555,
          0.7194444444444444,
          0.6972222222222223,
          0.7194444444444444,
          0.6972222222222223,
          0.7277777777777777,
          0.7124999999999999,
          0.7124999999999999,
          0.6986111111111111,
          0.6972222222222223,
          0.7569444444444444,
          0.7347222222222222,
          0.7124999999999999,
          0.7041666666666667,
          0.7124999999999999,
          0.7263888888888889,
          0.7124999999999999,
          0.7777777777777778,
          0.6833333333333333,
          0.7124999999999999,
          0.7055555555555555,
          0.7263888888888889,
          0.7277777777777777,
          0.6819444444444445,
          0.7277777777777777,
          0.7111111111111111,
          0.7055555555555555,
          0.7194444444444444,
          0.7416666666666667,
          0.7124999999999999,
          0.7277777777777777,
          0.6847222222222222,
          0.7361111111111112,
          0.7486111111111111,
          0.7347222222222222,
          0.7138888888888889,
          0.7111111111111111,
          0.7083333333333334,
          0.7277777777777777,
          0.7402777777777777,
          0.7222222222222222,
          0.6972222222222223,
          0.7208333333333333,
          0.7277777777777777,
          0.7208333333333333,
          0.7208333333333333,
          0.7055555555555555,
          0.7124999999999999,
          0.6986111111111111,
          0.6902777777777778,
          0.7708333333333334,
          0.7361111111111112,
          0.675,
          0.6763888888888889,
          0.7361111111111112,
          0.7055555555555555,
          0.7347222222222222,
          0.7486111111111111,
          0.7111111111111111,
          0.7208333333333333,
          0.7194444444444444,
          0.6902777777777778,
          0.7138888888888889,
          0.6833333333333333,
          0.7361111111111112,
          0.7430555555555556,
          0.7347222222222222,
          0.7277777777777777,
          0.7277777777777777,
          0.675,
          0.7430555555555556,
          0.7055555555555555,
          0.6972222222222223,
          0.7055555555555555,
          0.6902777777777778,
          0.7777777777777778,
          0.7222222222222222,
          0.6833333333333333,
          0.7277777777777777,
          0.7194444444444444,
          0.7138888888888889,
          0.7055555555555555,
          0.7138888888888889,
          0.6888888888888889,
          0.7361111111111112,
          0.7361111111111112,
          0.7569444444444444,
          0.7208333333333333,
          0.7208333333333333,
          0.7208333333333333,
          0.7208333333333333,
          0.7055555555555555,
          0.6833333333333333,
          0.6819444444444445,
          0.7416666666666667,
          0.6972222222222223,
          0.7069444444444444,
          0.7277777777777777,
          0.7111111111111111,
          0.7222222222222222,
          0.7361111111111112,
          0.75,
          0.6916666666666667,
          0.7430555555555556,
          0.6472222222222223,
          0.7194444444444444,
          0.75,
          0.7430555555555556,
          0.6861111111111111,
          0.7569444444444444,
          0.7069444444444444,
          0.7347222222222222,
          0.7138888888888889,
          0.7430555555555556,
          0.7347222222222222,
          0.7569444444444444,
          0.6833333333333333,
          0.6986111111111111,
          0.7222222222222222,
          0.7208333333333333,
          0.7152777777777778,
          0.6930555555555555,
          0.6916666666666667,
          0.7347222222222222,
          0.7055555555555555,
          0.7361111111111112,
          0.6833333333333333,
          0.7055555555555555,
          0.6972222222222223,
          0.7277777777777777,
          0.6902777777777778,
          0.7124999999999999,
          0.7347222222222222,
          0.7277777777777777,
          0.7069444444444444,
          0.6819444444444445,
          0.7124999999999999,
          0.7055555555555555,
          0.7,
          0.7208333333333333,
          0.7,
          0.7638888888888888,
          0.7208333333333333,
          0.7222222222222222,
          0.7472222222222222,
          0.7291666666666666,
          0.7138888888888889,
          0.6555555555555556,
          0.7361111111111112,
          0.7124999999999999,
          0.7208333333333333,
          0.7277777777777777,
          0.7291666666666666,
          0.7013888888888888,
          0.6986111111111111,
          0.7638888888888888,
          0.7124999999999999,
          0.7555555555555555,
          0.7347222222222222,
          0.7208333333333333,
          0.6708333333333333,
          0.7152777777777778,
          0.7347222222222222,
          0.7138888888888889,
          0.7430555555555556,
          0.7347222222222222,
          0.7277777777777777,
          0.7041666666666667,
          0.7208333333333333,
          0.7638888888888888,
          0.6888888888888889,
          0.6833333333333333,
          0.7055555555555555,
          0.6972222222222223,
          0.6986111111111111,
          0.7055555555555555,
          0.7291666666666666,
          0.75,
          0.7208333333333333,
          0.7430555555555556,
          0.7347222222222222,
          0.6986111111111111,
          0.7041666666666667,
          0.7416666666666667,
          0.7124999999999999,
          0.7124999999999999,
          0.7055555555555555,
          0.6986111111111111,
          0.6986111111111111,
          0.7277777777777777,
          0.7124999999999999,
          0.7430555555555556,
          0.7124999999999999,
          0.6916666666666667,
          0.7124999999999999,
          0.7208333333333333,
          0.7,
          0.7069444444444444,
          0.7194444444444444,
          0.6916666666666667,
          0.6930555555555555,
          0.7222222222222222,
          0.7138888888888889,
          0.7055555555555555,
          0.7055555555555555,
          0.7277777777777777,
          0.7055555555555555,
          0.7347222222222222,
          0.7124999999999999,
          0.7138888888888889,
          0.7416666666666667,
          0.7277777777777777,
          0.7361111111111112,
          0.7069444444444444,
          0.7277777777777777,
          0.7124999999999999,
          0.7569444444444444,
          0.6902777777777778,
          0.7277777777777777,
          0.7194444444444444,
          0.6930555555555555,
          0.7208333333333333,
          0.6986111111111111,
          0.75,
          0.6916666666666667,
          0.7,
          0.7055555555555555,
          0.7208333333333333,
          0.7347222222222222,
          0.7263888888888889,
          0.7361111111111112,
          0.6986111111111111,
          0.7638888888888888,
          0.7486111111111111,
          0.7277777777777777,
          0.7069444444444444,
          0.7208333333333333
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "title": {
          "text": "variable"
         },
         "tracegroupgap": 0
        },
        "margin": {
         "t": 60
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "index"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "value"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = px.line({'acc': acc_graph, 'val acc': val_acc_graph})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0.75\n",
      "loss:  tensor(6.7898, grad_fn=<BinaryCrossEntropyBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for batch, (x, y) in enumerate(test_dataloader):\n",
    "    print(batch)\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "\n",
    "    pred = model(x)\n",
    "    # print('pred: ', pred)\n",
    "    # print('===========')\n",
    "    # print('pred>0.5: ', pred>0.5)\n",
    "    # print('===========')\n",
    "    # print('y: ', y)\n",
    "    # print('===========')\n",
    "    print(calculate_acc(pred, y))\n",
    "    print('loss: ', loss_fn(pred, y))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
